{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import keras \n",
    "import keras.backend as K\n",
    "from scipy.signal import resample\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from collections import Counter\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model, load_model, model_from_json \n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, Concatenate,  Dropout \n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "def get_class_weights(y):\n",
    "    counter = Counter(y)\n",
    "    majority = max(counter.values())\n",
    "    return  {cls: float(majority/count) for cls, count in counter.items()}\n",
    "\n",
    "\n",
    "\n",
    "class Estimator:\n",
    "    l2p = 0.001\n",
    "    @staticmethod\n",
    "    def early_layers(inp, fm = (1,3), hid_act_func=\"relu\"):\n",
    "        # Start\n",
    "        x = Conv2D(32, fm, padding=\"same\", kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(inp)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(1, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        \n",
    "        # 1\n",
    "        x = Conv2D(32, fm, padding=\"same\", kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(1, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def late_layers(inp, num_classes, fm = (1,3), act_func=\"softmax\", hid_act_func=\"relu\", b_name=\"Identifier\"):\n",
    "        # 2\n",
    "        x = Conv2D(32, fm, padding=\"same\", kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(inp)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(1, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        \n",
    "        \n",
    "        # End\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(128, kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(32, kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(num_classes, activation=act_func, name = b_name)(x)\n",
    "\n",
    "        return x\n",
    "   \n",
    "    @staticmethod\n",
    "    def build(height, width, num_classes, name, fm = (1,3), act_func=\"softmax\",hid_act_func=\"relu\"):\n",
    "        inp = Input(shape=(height, width, 1))\n",
    "        early = Estimator.early_layers(inp, fm, hid_act_func=hid_act_func)\n",
    "        late  = Estimator.late_layers(early, num_classes, fm, act_func=act_func, hid_act_func=hid_act_func)\n",
    "        model = Model(inputs=inp, outputs=late ,name=name)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_ds_infos():\n",
    "    \"\"\"\n",
    "    Read the file includes data subject information.\n",
    "    \n",
    "    Data Columns:\n",
    "    0: code [1-24]\n",
    "    1: weight [kg]\n",
    "    2: height [cm]\n",
    "    3: age [years]\n",
    "    4: gender [0:Female, 1:Male]\n",
    "    \n",
    "    Returns:\n",
    "        A pandas DataFrame that contains inforamtion about data subjects' attributes \n",
    "    \"\"\" \n",
    "\n",
    "    dss = pd.read_csv(\"data_subjects_info.csv\")\n",
    "    print(\"[INFO] -- Data subjects' information is imported.\")\n",
    "    \n",
    "    return dss\n",
    "\n",
    "def set_data_types(data_types=[\"userAcceleration\"]):\n",
    "    \"\"\"\n",
    "    Select the sensors and the mode to shape the final dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_types: A list of sensor data type from this list: [attitude, gravity, rotationRate, userAcceleration] \n",
    "\n",
    "    Returns:\n",
    "        It returns a list of columns to use for creating time-series from files.\n",
    "    \"\"\"\n",
    "    dt_list = []\n",
    "    for t in data_types:\n",
    "        if t != \"attitude\":\n",
    "            dt_list.append([t+\".x\",t+\".y\",t+\".z\"])\n",
    "        else:\n",
    "            dt_list.append([t+\".roll\", t+\".pitch\", t+\".yaw\"])\n",
    "\n",
    "    return dt_list\n",
    "\n",
    "\n",
    "def creat_time_series(dt_list, act_labels, trial_codes, mode=\"mag\", labeled=True, combine_grav_acc=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dt_list: A list of columns that shows the type of data we want.\n",
    "        act_labels: list of activites\n",
    "        trial_codes: list of trials\n",
    "        mode: It can be \"raw\" which means you want raw data\n",
    "        for every dimention of each data type,\n",
    "        [attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)].\n",
    "        or it can be \"mag\" which means you only want the magnitude for each data type: (x^2+y^2+z^2)^(1/2)\n",
    "        labeled: True, if we want a labeld dataset. False, if we only want sensor values.\n",
    "        combine_grav_acc: True, means adding each axis of gravity to  corresponding axis of userAcceleration.\n",
    "    Returns: \n",
    "        It returns a time-series of sensor data.\n",
    "    \n",
    "    \"\"\"\n",
    "    num_data_cols = len(dt_list) if mode == \"mag\" else len(dt_list*3)\n",
    "\n",
    "    if labeled:\n",
    "        dataset = np.zeros((0,num_data_cols+7)) # \"7\" --> [act, code, weight, height, age, gender, trial] \n",
    "    else:\n",
    "        dataset = np.zeros((0,num_data_cols))\n",
    "        \n",
    "    ds_list = get_ds_infos()\n",
    "    \n",
    "    print(\"[INFO] -- Creating Time-Series\")\n",
    "    for sub_id in ds_list[\"code\"]:\n",
    "        for act_id, act in enumerate(act_labels):\n",
    "            for trial in trial_codes[act_id]:\n",
    "                fname = 'A_DeviceMotion_data/'+act+'_'+str(trial)+'/sub_'+str(int(sub_id))+'.csv'\n",
    "                raw_data = pd.read_csv(fname)\n",
    "                raw_data = raw_data.drop(['Unnamed: 0'], axis=1)\n",
    "                vals = np.zeros((len(raw_data), num_data_cols))\n",
    "                \n",
    "                if combine_grav_acc:\n",
    "                    raw_data[\"userAcceleration.x\"] = raw_data[\"userAcceleration.x\"].add(raw_data[\"gravity.x\"])\n",
    "                    raw_data[\"userAcceleration.y\"] = raw_data[\"userAcceleration.y\"].add(raw_data[\"gravity.y\"])\n",
    "                    raw_data[\"userAcceleration.z\"] = raw_data[\"userAcceleration.z\"].add(raw_data[\"gravity.z\"])\n",
    "                \n",
    "                for x_id, axes in enumerate(dt_list):\n",
    "                    if mode == \"mag\":\n",
    "                        vals[:,x_id] = (raw_data[axes]**2).sum(axis=1)**0.5        \n",
    "                    else:\n",
    "                        vals[:,x_id*3:(x_id+1)*3] = raw_data[axes].values\n",
    "                    vals = vals[:,:num_data_cols]\n",
    "                if labeled:\n",
    "                    lbls = np.array([[act_id,\n",
    "                            sub_id-1,\n",
    "                            ds_list[\"weight\"][sub_id-1],\n",
    "                            ds_list[\"height\"][sub_id-1],\n",
    "                            ds_list[\"age\"][sub_id-1],\n",
    "                            ds_list[\"gender\"][sub_id-1],\n",
    "                            trial          \n",
    "                           ]]*len(raw_data))\n",
    "                    vals = np.concatenate((vals, lbls), axis=1)\n",
    "                dataset = np.append(dataset,vals, axis=0)\n",
    "    cols = []\n",
    "    for axes in dt_list:\n",
    "        if mode == \"raw\":\n",
    "            cols += axes\n",
    "        else:\n",
    "            cols += [str(axes[0][:-2])]\n",
    "            \n",
    "    if labeled:\n",
    "        cols += [\"act\", \"id\", \"weight\", \"height\", \"age\", \"gender\", \"trial\"]\n",
    "    \n",
    "    dataset = pd.DataFrame(data=dataset, columns=cols)\n",
    "    return dataset\n",
    "#________________________________\n",
    "#________________________________\n",
    "\n",
    "def ts_to_secs(dataset, w, s, standardize = False, **options):\n",
    "    \n",
    "    data = dataset[dataset.columns[:-7]].values    \n",
    "    act_labels = dataset[\"act\"].values\n",
    "    id_labels = dataset[\"id\"].values\n",
    "    trial_labels = dataset[\"trial\"].values\n",
    "\n",
    "    mean = 0\n",
    "    std = 1\n",
    "    if standardize:\n",
    "        ## Standardize each sensorâ€™s data to have a zero mean and unity standard deviation.\n",
    "        ## As usual, we normalize test dataset by training dataset's parameters \n",
    "        if options:\n",
    "            mean = options.get(\"mean\")\n",
    "            std = options.get(\"std\")\n",
    "            print(\"[INFO] -- Test Data has been standardized\")\n",
    "        else:\n",
    "            mean = data.mean(axis=0)\n",
    "            std = data.std(axis=0)\n",
    "            print(\"[INFO] -- Training Data has been standardized: the mean is = \"+str(mean)+\" ; and the std is = \"+str(std))            \n",
    "\n",
    "        data -= mean\n",
    "        data /= std\n",
    "    else:\n",
    "        print(\"[INFO] -- Without Standardization.....\")\n",
    "\n",
    "    ## We want the Rows of matrices show each Feature and the Columns show time points.\n",
    "    data = data.T\n",
    "\n",
    "    m = data.shape[0]   # Data Dimension \n",
    "    ttp = data.shape[1] # Total Time Points\n",
    "    number_of_secs = int(round(((ttp - w)/s)))\n",
    "\n",
    "    ##  Create a 3D matrix for Storing Sections  \n",
    "    secs_data = np.zeros((number_of_secs , m , w ))\n",
    "    act_secs_labels = np.zeros(number_of_secs)\n",
    "    id_secs_labels = np.zeros(number_of_secs)\n",
    "\n",
    "    k=0\n",
    "    for i in range(0 , ttp-w, s):\n",
    "        j = i // s\n",
    "        if j >= number_of_secs:\n",
    "            break\n",
    "        if id_labels[i] != id_labels[i+w-1]: \n",
    "            continue\n",
    "        if act_labels[i] != act_labels[i+w-1]: \n",
    "            continue\n",
    "        if trial_labels[i] != trial_labels[i+w-1]:\n",
    "            continue\n",
    "            \n",
    "        secs_data[k] = data[:, i:i+w]\n",
    "        act_secs_labels[k] = act_labels[i].astype(int)\n",
    "        id_secs_labels[k] = id_labels[i].astype(int)\n",
    "        k = k+1\n",
    "        \n",
    "    secs_data = secs_data[0:k]\n",
    "    act_secs_labels = act_secs_labels[0:k]\n",
    "    id_secs_labels = id_secs_labels[0:k]\n",
    "    return secs_data, act_secs_labels, id_secs_labels, mean, std\n",
    "##________________________________________________________________\n",
    "\n",
    "\n",
    "ACT_LABELS = [\"dws\",\"ups\", \"wlk\", \"jog\", \"std\", \"sit\"]\n",
    "TRIAL_CODES = {\n",
    "    ACT_LABELS[0]:[1,2,11],\n",
    "    ACT_LABELS[1]:[3,4,12],\n",
    "    ACT_LABELS[2]:[7,8,15],\n",
    "    ACT_LABELS[3]:[9,16],\n",
    "    ACT_LABELS[4]:[6,14],\n",
    "    ACT_LABELS[5]:[5,13],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/45305384/5210098\n",
    "def f1_metric(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- Selected sensor data types: ['rotationRate', 'userAcceleration'] -- Mode: mag -- Grav+Acc: True\n",
      "[INFO] -- Selected activites: ['dws', 'ups', 'wlk', 'jog']\n",
      "[INFO] -- Data subjects' information is imported.\n",
      "[INFO] -- Creating Time-Series\n",
      "[INFO] -- Shape of time-Series dataset:(767660, 9)\n",
      "[INFO] -- Test IDs: [4, 9, 11, 21]\n",
      "[INFO] -- Shape of Train Time-Series :(646207, 9)\n",
      "[INFO] -- Shape of Test Time-Series :(121453, 9)\n",
      "___________Train_VAL____________\n",
      "[INFO] -- Training Time-Series :(523129, 9)\n",
      "[INFO] -- Validation Time-Series :(123078, 9)\n",
      "___________________________________________________\n",
      "   rotationRate  userAcceleration  act   id  weight  height   age  gender  \\\n",
      "0      1.370498          1.195847  0.0  0.0   102.0   188.0  46.0     1.0   \n",
      "1      1.141648          1.196990  0.0  0.0   102.0   188.0  46.0     1.0   \n",
      "2      0.372530          1.117437  0.0  0.0   102.0   188.0  46.0     1.0   \n",
      "3      1.049628          1.088320  0.0  0.0   102.0   188.0  46.0     1.0   \n",
      "4      0.921229          1.390551  0.0  0.0   102.0   188.0  46.0     1.0   \n",
      "\n",
      "   trial  \n",
      "0    1.0  \n",
      "1    1.0  \n",
      "2    1.0  \n",
      "3    1.0  \n",
      "4    1.0  \n",
      "[INFO] -- Training Data has been standardized: the mean is = [2.17728825 1.19431016] ; and the std is = [1.43229632 0.70168121]\n",
      "[INFO] -- Test Data has been standardized\n",
      "[INFO] -- Test Data has been standardized\n",
      "[INFO] -- Training Sections: (50532, 2, 128)\n",
      "[INFO] -- Validation Sections: (11288, 2, 128)\n",
      "[INFO] -- Test Sections:  (11589, 2, 128)\n"
     ]
    }
   ],
   "source": [
    "## Here we set parameter to build labeld time-series from dataset of \"(A)DeviceMotion_data\"\n",
    "## attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)\n",
    "results ={}\n",
    "sdt = [\"rotationRate\",\"userAcceleration\"]\n",
    "mode = \"mag\"\n",
    "cga = True # Add gravity to acceleration or not\n",
    "\n",
    "print(\"[INFO] -- Selected sensor data types: \"+str(sdt)+\" -- Mode: \"+str(mode)+\" -- Grav+Acc: \"+str(cga))    \n",
    "act_labels = ACT_LABELS [0:4]\n",
    "\n",
    "print(\"[INFO] -- Selected activites: \"+str(act_labels))    \n",
    "trial_codes = [TRIAL_CODES[act] for act in act_labels]\n",
    "dt_list = set_data_types(sdt)\n",
    "dataset = creat_time_series(dt_list, act_labels, trial_codes, mode=mode, labeled=True, combine_grav_acc = cga)\n",
    "print(\"[INFO] -- Shape of time-Series dataset:\"+str(dataset.shape))    \n",
    "\n",
    "\n",
    "#*****************\n",
    "TRAIN_TEST_TYPE = \"subject\" # \"subject\" or \"trial\"\n",
    "#*****************\n",
    "\n",
    "if TRAIN_TEST_TYPE == \"subject\":\n",
    "    test_ids = [4,9,11,21]\n",
    "    print(\"[INFO] -- Test IDs: \"+str(test_ids))\n",
    "    test_ts = dataset.loc[(dataset['id'].isin(test_ids))]\n",
    "    train_ts = dataset.loc[~(dataset['id'].isin(test_ids))]\n",
    "else:\n",
    "    test_trail = [11,12,13,14,15,16]  \n",
    "    print(\"[INFO] -- Test Trials: \"+str(test_trail))\n",
    "    test_ts = dataset.loc[(dataset['trial'].isin(test_trail))]\n",
    "    train_ts = dataset.loc[~(dataset['trial'].isin(test_trail))]\n",
    "\n",
    "print(\"[INFO] -- Shape of Train Time-Series :\"+str(train_ts.shape))\n",
    "print(\"[INFO] -- Shape of Test Time-Series :\"+str(test_ts.shape))\n",
    "\n",
    "print(\"___________Train_VAL____________\")\n",
    "val_trail = [11,12,13,14,15,16]\n",
    "val_ts = train_ts.loc[(train_ts['trial'].isin(val_trail))]\n",
    "train_ts = train_ts.loc[~(train_ts['trial'].isin(val_trail))]\n",
    "print(\"[INFO] -- Training Time-Series :\"+str(train_ts.shape))\n",
    "print(\"[INFO] -- Validation Time-Series :\"+str(val_ts.shape))\n",
    "\n",
    "print(\"___________________________________________________\")\n",
    "print(train_ts.head())\n",
    "\n",
    "## This Variable Defines the Size of Sliding Window\n",
    "## ( e.g. 100 means in each snapshot we just consider 100 consecutive observations of each sensor) \n",
    "w = 128 # 50 Equals to 1 second for MotionSense Dataset (it is on 50Hz samplig rate)\n",
    "## Here We Choose Step Size for Building Diffrent Snapshots from Time-Series Data\n",
    "## ( smaller step size will increase the amount of the instances and higher computational cost may be incurred )\n",
    "s = 10\n",
    "train_data, act_train, id_train, train_mean, train_std = ts_to_secs(train_ts.copy(),\n",
    "                                                                   w,\n",
    "                                                                   s,\n",
    "                                                                   standardize = True)\n",
    "\n",
    "s = 10\n",
    "val_data, act_val, id_val, val_mean, val_std = ts_to_secs(val_ts.copy(),\n",
    "                                                          w,\n",
    "                                                          s,\n",
    "                                                          standardize = True,\n",
    "                                                          mean = train_mean, \n",
    "                                                          std = train_std)\n",
    "\n",
    "s = 10\n",
    "test_data, act_test, id_test, test_mean, test_std = ts_to_secs(test_ts.copy(),\n",
    "                                                              w,\n",
    "                                                              s,\n",
    "                                                              standardize = True,\n",
    "                                                              mean = train_mean, \n",
    "                                                              std = train_std)\n",
    "\n",
    "print(\"[INFO] -- Training Sections: \"+str(train_data.shape))\n",
    "print(\"[INFO] -- Validation Sections: \"+str(val_data.shape))\n",
    "print(\"[INFO] -- Test Sections:  \"+str(test_data.shape))\n",
    "\n",
    "\n",
    "id_train_labels = to_categorical(id_train)\n",
    "id_val_labels = to_categorical(id_val)\n",
    "id_test_labels = to_categorical(id_test)\n",
    "\n",
    "act_train_labels = to_categorical(act_train)\n",
    "act_val_labels = to_categorical(act_val)\n",
    "act_test_labels = to_categorical(act_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bck_train_data = train_data.copy()\n",
    "bck_val_data = val_data.copy()\n",
    "bck_test_data = test_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_act_id(sdt, mode, ep, cga, num_sampels):\n",
    "    global bck_train_data, bck_val_data, bck_test_data\n",
    "    global act_train_labels,  act_val_labels,  act_test_labels\n",
    "    ## DownSampling ##\n",
    "    if  num_sampels!= 128:\n",
    "        ds_train_data = bck_train_data.copy()\n",
    "        ds_val_data = bck_val_data.copy()\n",
    "        ds_test_data = bck_test_data.copy()\n",
    "    \n",
    "        for sens in range(2):\n",
    "            tmp = np.array([resample(x,num_sampels) for x in ds_train_data[:,sens,:]])\n",
    "            ds_train_data[:,sens,:num_sampels] = tmp\n",
    "            \n",
    "            tmp = np.array([resample(x,num_sampels) for x in ds_val_data[:,sens,:]])\n",
    "            ds_val_data[:,sens,:num_sampels] = tmp\n",
    "\n",
    "            tmp = np.array([resample(x,num_sampels) for x in ds_test_data[:,sens,:]])\n",
    "            ds_test_data[:,sens,:num_sampels] = tmp \n",
    "\n",
    "        ds_train_data = ds_train_data[:,:,:num_sampels]\n",
    "        ds_val_data = ds_val_data[:,:,:num_sampels]\n",
    "        ds_test_data = ds_test_data[:,:,:num_sampels]\n",
    "\n",
    "        print(\"[INFO] -- Training Sections:\", ds_train_data.shape)\n",
    "        print(\"[INFO] -- Validation Sections:\", ds_val_data.shape)\n",
    "        print(\"[INFO] -- Test Sections:\", ds_test_data.shape)\n",
    "\n",
    "        train_data = ds_train_data\n",
    "        val_data = ds_val_data\n",
    "        test_data = ds_test_data\n",
    "        width = num_sampels\n",
    "    else:\n",
    "        train_data = bck_train_data.copy()\n",
    "        val_data = bck_val_data.copy()\n",
    "        test_data = bck_test_data.copy()\n",
    "\n",
    "    height = train_data.shape[1]\n",
    "    width = train_data.shape[2]\n",
    "    ## Here we add an extra dimension to the datasets just to be ready for using with Convolution2D\n",
    "    train_data = np.expand_dims(train_data,axis=3)\n",
    "    print(\"[INFO] -- Training Sections:\", train_data.shape)\n",
    "    val_data = np.expand_dims(val_data,axis=3)\n",
    "    print(\"[INFO] -- Validation Sections:\", val_data.shape)\n",
    "    test_data = np.expand_dims(test_data,axis=3)\n",
    "    print(\"[INFO] -- Test Sections:\", test_data.shape)\n",
    "\n",
    "   \n",
    "\n",
    "    height = train_data.shape[1]\n",
    "    width = train_data.shape[2]\n",
    "\n",
    "    id_class_numbers = 24\n",
    "    act_class_numbers = 4\n",
    "    fm = (2,5)\n",
    "\n",
    "    print(\"___________________________________________________\")\n",
    "    ## Callbacks\n",
    "    eval_metric= \"val_f1_metric\"    \n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor=eval_metric, mode='max', patience = 7)\n",
    "    filepath=\"XXACT.best.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor=eval_metric, verbose=0, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint, early_stop]\n",
    "    ## Callbacks\n",
    "\n",
    "    eval_act = Estimator.build(height, width, act_class_numbers, name =\"EVAL_ACT\", fm=fm, act_func=\"softmax\",hid_act_func=\"relu\")\n",
    "    eval_act.compile( loss=\"categorical_crossentropy\", optimizer='adam', metrics=['acc',f1_metric])\n",
    "    print(\"Model Size = \"+str(eval_act.count_params()))\n",
    "    #print(eval_act.summary())\n",
    "    eval_act.fit(train_data, act_train_labels,\n",
    "                validation_data = (val_data, act_val_labels),\n",
    "                epochs = ep,\n",
    "                batch_size = 128,\n",
    "                verbose = 0,\n",
    "                class_weight = get_class_weights(np.argmax(act_train_labels,axis=1)),\n",
    "                callbacks = callbacks_list\n",
    "               )\n",
    "\n",
    "    eval_act.load_weights(\"XXACT.best.hdf5\")\n",
    "    eval_act.compile( loss=\"categorical_crossentropy\", optimizer='adam', metrics=['acc',f1_metric])\n",
    "\n",
    "    result1 = eval_act.evaluate(test_data, act_test_labels, verbose = 2)\n",
    "    act_acc = result1[1].round(4)*100\n",
    "    print(\"***[RESULT]*** ACT Accuracy: \"+str(act_acc))\n",
    "\n",
    "    preds = eval_act.predict(test_data)\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    conf_mat = confusion_matrix(np.argmax(act_test_labels, axis=1), preds)\n",
    "    conf_mat = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "    print(\"***[RESULT]*** ACT  Confusion Matrix\")\n",
    "    print(np.array(conf_mat).round(3)*100)  \n",
    "\n",
    "    f1act = f1_score(np.argmax(act_test_labels, axis=1), preds, average=None).mean()\n",
    "    print(\"***[RESULT]*** ACT Averaged F-1 Score : \"+str(f1act))\n",
    "\n",
    "    return f1act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- For Sample Rate: 50 --> Number of Samples: 128\n",
      "[INFO] -- Training Sections: (50532, 2, 128, 1)\n",
      "[INFO] -- Validation Sections: (11288, 2, 128, 1)\n",
      "[INFO] -- Test Sections: (11589, 2, 128, 1)\n",
      "___________________________________________________\n",
      "Model Size = 157380\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2, 128, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 2, 128, 32)        352       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2, 128, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 2, 64, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 64, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 2, 64, 32)         10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2, 64, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 32, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2, 32, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 2, 32, 32)         10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 2, 32, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 16, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 16, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Identifier (Dense)           (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 157,380\n",
      "Trainable params: 156,868\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#*******************\n",
    "sample_rate = 50 #Hz\n",
    "#*******************\n",
    "for i in range(3):\n",
    "    num_sampels = (128*sample_rate)//50\n",
    "    print(\"[INFO] -- For Sample Rate: \"+str(sample_rate)+\" --> Number of Samples: \"+str(num_sampels))\n",
    "    results[sample_rate,i] = eval_act_id(sdt, mode, ep, cga, num_sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- For Sample Rate: 45 --> Number of Samples: 115\n",
      "[INFO] -- Training Sections: (50532, 2, 115)\n",
      "[INFO] -- Validation Sections: (11288, 2, 115)\n",
      "[INFO] -- Test Sections: (11589, 2, 115)\n",
      "[INFO] -- Training Sections: (50532, 2, 115, 1)\n",
      "[INFO] -- Validation Sections: (11288, 2, 115, 1)\n",
      "[INFO] -- Test Sections: (11589, 2, 115, 1)\n",
      "___________________________________________________\n",
      "Model Size = 140996\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.96214, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.96214 to 0.96955, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00003: val_f1_metric did not improve from 0.96955\n",
      "\n",
      "Epoch 00004: val_f1_metric improved from 0.96955 to 0.98005, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00005: val_f1_metric improved from 0.98005 to 0.98616, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00006: val_f1_metric did not improve from 0.98616\n",
      "\n",
      "Epoch 00007: val_f1_metric did not improve from 0.98616\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.98616\n",
      "\n",
      "Epoch 00009: val_f1_metric improved from 0.98616 to 0.98865, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.98865\n",
      "\n",
      "Epoch 00011: val_f1_metric improved from 0.98865 to 0.99186, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.99186\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.99186\n",
      "\n",
      "Epoch 00014: val_f1_metric did not improve from 0.99186\n",
      "\n",
      "Epoch 00015: val_f1_metric did not improve from 0.99186\n",
      "\n",
      "Epoch 00016: val_f1_metric did not improve from 0.99186\n",
      "\n",
      "Epoch 00017: val_f1_metric did not improve from 0.99186\n",
      "\n",
      "Epoch 00018: val_f1_metric did not improve from 0.99186\n",
      "***[RESULT]*** ACT Accuracy: 91.42\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[95.7  1.6  2.5  0.2]\n",
      " [ 1.9 95.8  2.4  0. ]\n",
      " [11.4  0.8 87.8  0. ]\n",
      " [ 8.2  0.   0.  91.8]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 0.9115457397973644\n"
     ]
    }
   ],
   "source": [
    "#*******************\n",
    "sample_rate = 45 #Hz\n",
    "#*******************\n",
    "num_sampels = (128*sample_rate)//50\n",
    "print(\"[INFO] -- For Sample Rate: \"+str(sample_rate)+\" --> Number of Samples: \"+str(num_sampels))\n",
    "results[sample_rate] = eval_act_id(sdt, mode, ep, cga, num_sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- For Sample Rate: 40 --> Number of Samples: 102\n",
      "[INFO] -- Training Sections: (50532, 2, 102)\n",
      "[INFO] -- Validation Sections: (11288, 2, 102)\n",
      "[INFO] -- Test Sections: (11589, 2, 102)\n",
      "[INFO] -- Training Sections: (50532, 2, 102, 1)\n",
      "[INFO] -- Validation Sections: (11288, 2, 102, 1)\n",
      "[INFO] -- Test Sections: (11589, 2, 102, 1)\n",
      "___________________________________________________\n",
      "Model Size = 124612\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.94469, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.94469 to 0.96134, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00003: val_f1_metric improved from 0.96134 to 0.97950, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00004: val_f1_metric did not improve from 0.97950\n",
      "\n",
      "Epoch 00005: val_f1_metric did not improve from 0.97950\n",
      "\n",
      "Epoch 00006: val_f1_metric improved from 0.97950 to 0.98234, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00007: val_f1_metric improved from 0.98234 to 0.98247, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00008: val_f1_metric improved from 0.98247 to 0.98910, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.98910\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.98910\n",
      "\n",
      "Epoch 00011: val_f1_metric improved from 0.98910 to 0.99017, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.99017\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.99017\n",
      "\n",
      "Epoch 00014: val_f1_metric did not improve from 0.99017\n",
      "\n",
      "Epoch 00015: val_f1_metric did not improve from 0.99017\n",
      "\n",
      "Epoch 00016: val_f1_metric did not improve from 0.99017\n",
      "\n",
      "Epoch 00017: val_f1_metric did not improve from 0.99017\n",
      "\n",
      "Epoch 00018: val_f1_metric did not improve from 0.99017\n",
      "***[RESULT]*** ACT Accuracy: 91.53999999999999\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[93.9  0.5  3.7  2. ]\n",
      " [ 3.4 88.5  8.1  0. ]\n",
      " [10.5  0.6 88.9  0. ]\n",
      " [ 0.4  0.1  0.  99.4]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 0.9149528499781788\n"
     ]
    }
   ],
   "source": [
    "#*******************\n",
    "sample_rate = 40 #Hz\n",
    "#*******************\n",
    "num_sampels = (128*sample_rate)//50\n",
    "print(\"[INFO] -- For Sample Rate: \"+str(sample_rate)+\" --> Number of Samples: \"+str(num_sampels))\n",
    "results[sample_rate] = eval_act_id(sdt, mode, ep, cga, num_sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- For Sample Rate: 35 --> Number of Samples: 89\n",
      "[INFO] -- Training Sections: (50532, 2, 89)\n",
      "[INFO] -- Validation Sections: (11288, 2, 89)\n",
      "[INFO] -- Test Sections: (11589, 2, 89)\n",
      "[INFO] -- Training Sections: (50532, 2, 89, 1)\n",
      "[INFO] -- Validation Sections: (11288, 2, 89, 1)\n",
      "[INFO] -- Test Sections: (11589, 2, 89, 1)\n",
      "___________________________________________________\n",
      "Model Size = 116420\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.96839, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.96839 to 0.97717, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00003: val_f1_metric improved from 0.97717 to 0.97735, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00004: val_f1_metric improved from 0.97735 to 0.98272, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00005: val_f1_metric improved from 0.98272 to 0.98993, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00006: val_f1_metric did not improve from 0.98993\n",
      "\n",
      "Epoch 00007: val_f1_metric improved from 0.98993 to 0.99283, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.99283\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.99283\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.99283\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.99283\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.99283\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.99283\n",
      "\n",
      "Epoch 00014: val_f1_metric did not improve from 0.99283\n",
      "***[RESULT]*** ACT Accuracy: 91.86\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[96.1  1.4  2.2  0.2]\n",
      " [ 0.9 97.1  2.   0. ]\n",
      " [13.4  1.4 85.2  0. ]\n",
      " [ 1.   0.3  0.  98.7]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 0.9209086518048157\n"
     ]
    }
   ],
   "source": [
    "#*******************\n",
    "sample_rate = 35 #Hz\n",
    "#*******************\n",
    "num_sampels = (128*sample_rate)//50\n",
    "print(\"[INFO] -- For Sample Rate: \"+str(sample_rate)+\" --> Number of Samples: \"+str(num_sampels))\n",
    "results[sample_rate] = eval_act_id(sdt, mode, ep, cga, num_sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- For Sample Rate: 30 --> Number of Samples: 76\n",
      "[INFO] -- Training Sections: (50532, 2, 76)\n",
      "[INFO] -- Validation Sections: (11288, 2, 76)\n",
      "[INFO] -- Test Sections: (11589, 2, 76)\n",
      "[INFO] -- Training Sections: (50532, 2, 76, 1)\n",
      "[INFO] -- Validation Sections: (11288, 2, 76, 1)\n",
      "[INFO] -- Test Sections: (11589, 2, 76, 1)\n",
      "___________________________________________________\n",
      "Model Size = 100036\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.96512, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.96512 to 0.98417, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00003: val_f1_metric did not improve from 0.98417\n",
      "\n",
      "Epoch 00004: val_f1_metric did not improve from 0.98417\n",
      "\n",
      "Epoch 00005: val_f1_metric improved from 0.98417 to 0.98591, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00006: val_f1_metric improved from 0.98591 to 0.99256, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00007: val_f1_metric did not improve from 0.99256\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.99256\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.99256\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.99256\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.99256\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.99256\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.99256\n",
      "***[RESULT]*** ACT Accuracy: 93.56\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[96.   2.1  1.8  0.1]\n",
      " [ 0.9 97.4  1.7  0. ]\n",
      " [ 8.7  2.7 88.6  0. ]\n",
      " [ 0.6  0.   0.  99.4]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 0.9360753836586639\n"
     ]
    }
   ],
   "source": [
    "#*******************\n",
    "sample_rate = 30 #Hz\n",
    "#*******************\n",
    "num_sampels = (128*sample_rate)//50\n",
    "print(\"[INFO] -- For Sample Rate: \"+str(sample_rate)+\" --> Number of Samples: \"+str(num_sampels))\n",
    "results[sample_rate] = eval_act_id(sdt, mode, ep, cga, num_sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] --  For Sample Rate: 25 --> Number of Samples: 64\n",
      "[INFO] -- Training Sections: (50532, 2, 64)\n",
      "[INFO] -- Validation Sections: (11288, 2, 64)\n",
      "[INFO] -- Test Sections: (11589, 2, 64)\n",
      "[INFO] -- Training Sections: (50532, 2, 64, 1)\n",
      "[INFO] -- Validation Sections: (11288, 2, 64, 1)\n",
      "[INFO] -- Test Sections: (11589, 2, 64, 1)\n",
      "___________________________________________________\n",
      "Model Size = 91844\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.96554, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00002: val_f1_metric did not improve from 0.96554\n",
      "\n",
      "Epoch 00003: val_f1_metric improved from 0.96554 to 0.97176, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00004: val_f1_metric improved from 0.97176 to 0.98562, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00005: val_f1_metric improved from 0.98562 to 0.98689, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00006: val_f1_metric improved from 0.98689 to 0.98749, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00007: val_f1_metric did not improve from 0.98749\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.98749\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.98749\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.98749\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.98749\n",
      "\n",
      "Epoch 00012: val_f1_metric improved from 0.98749 to 0.98808, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00013: val_f1_metric improved from 0.98808 to 0.98832, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00014: val_f1_metric did not improve from 0.98832\n",
      "\n",
      "Epoch 00015: val_f1_metric did not improve from 0.98832\n",
      "\n",
      "Epoch 00016: val_f1_metric did not improve from 0.98832\n",
      "\n",
      "Epoch 00017: val_f1_metric did not improve from 0.98832\n",
      "\n",
      "Epoch 00018: val_f1_metric improved from 0.98832 to 0.98918, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00019: val_f1_metric improved from 0.98918 to 0.99051, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00020: val_f1_metric did not improve from 0.99051\n",
      "\n",
      "Epoch 00021: val_f1_metric did not improve from 0.99051\n",
      "\n",
      "Epoch 00022: val_f1_metric did not improve from 0.99051\n",
      "\n",
      "Epoch 00023: val_f1_metric did not improve from 0.99051\n",
      "\n",
      "Epoch 00024: val_f1_metric did not improve from 0.99051\n",
      "\n",
      "Epoch 00025: val_f1_metric did not improve from 0.99051\n",
      "\n",
      "Epoch 00026: val_f1_metric did not improve from 0.99051\n",
      "***[RESULT]*** ACT Accuracy: 95.69\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[95.6  0.8  3.3  0.3]\n",
      " [ 1.2 91.8  7.   0. ]\n",
      " [ 3.3  0.7 96.   0. ]\n",
      " [ 0.2  0.4  0.  99.3]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 0.9560311043695089\n"
     ]
    }
   ],
   "source": [
    "#*******************\n",
    "sample_rate = 25 #Hz\n",
    "#*******************\n",
    "num_sampels = (128*sample_rate)//50\n",
    "print(\"[INFO] --  For Sample Rate: \"+str(sample_rate)+\" --> Number of Samples: \"+str(num_sampels))\n",
    "results[sample_rate] = eval_act_id(sdt, mode, ep, cga, num_sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- For Sample Rate: 20 --> Number of Samples: 51\n",
      "[INFO] -- Training Sections: (50532, 2, 51)\n",
      "[INFO] -- Validation Sections: (11288, 2, 51)\n",
      "[INFO] -- Test Sections: (11589, 2, 51)\n",
      "[INFO] -- Training Sections: (50532, 2, 51, 1)\n",
      "[INFO] -- Validation Sections: (11288, 2, 51, 1)\n",
      "[INFO] -- Test Sections: (11589, 2, 51, 1)\n",
      "___________________________________________________\n",
      "Model Size = 75460\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.96663, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.96663 to 0.97998, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00003: val_f1_metric improved from 0.97998 to 0.98081, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00004: val_f1_metric did not improve from 0.98081\n",
      "\n",
      "Epoch 00005: val_f1_metric improved from 0.98081 to 0.99004, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00006: val_f1_metric did not improve from 0.99004\n",
      "\n",
      "Epoch 00007: val_f1_metric did not improve from 0.99004\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.99004\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.99004\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.99004\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.99004\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.99004\n",
      "***[RESULT]*** ACT Accuracy: 91.72\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[94.6  1.9  3.5  0. ]\n",
      " [ 2.3 89.3  8.4  0. ]\n",
      " [ 9.4  1.6 88.9  0. ]\n",
      " [ 0.8  0.3  0.  98.8]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 0.9175323455727964\n"
     ]
    }
   ],
   "source": [
    "#*******************\n",
    "sample_rate = 20 #Hz\n",
    "#*******************\n",
    "num_sampels = (128*sample_rate)//50\n",
    "print(\"[INFO] -- For Sample Rate: \"+str(sample_rate)+\" --> Number of Samples: \"+str(num_sampels))\n",
    "results[sample_rate] = eval_act_id(sdt, mode, ep, cga, num_sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- For Sample Rate: 15 --> Number of Samples: 38\n",
      "[INFO] -- Training Sections: (50532, 2, 38)\n",
      "[INFO] -- Validation Sections: (11288, 2, 38)\n",
      "[INFO] -- Test Sections: (11589, 2, 38)\n",
      "[INFO] -- Training Sections: (50532, 2, 38, 1)\n",
      "[INFO] -- Validation Sections: (11288, 2, 38, 1)\n",
      "[INFO] -- Test Sections: (11589, 2, 38, 1)\n",
      "___________________________________________________\n",
      "Model Size = 59076\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.96347, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.96347 to 0.98483, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00003: val_f1_metric did not improve from 0.98483\n",
      "\n",
      "Epoch 00004: val_f1_metric improved from 0.98483 to 0.98879, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00005: val_f1_metric improved from 0.98879 to 0.98955, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00006: val_f1_metric improved from 0.98955 to 0.98956, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00007: val_f1_metric improved from 0.98956 to 0.99082, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00008: val_f1_metric improved from 0.99082 to 0.99189, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.99189\n",
      "\n",
      "Epoch 00010: val_f1_metric improved from 0.99189 to 0.99195, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.99195\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.99195\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.99195\n",
      "\n",
      "Epoch 00014: val_f1_metric did not improve from 0.99195\n",
      "\n",
      "Epoch 00015: val_f1_metric did not improve from 0.99195\n",
      "\n",
      "Epoch 00016: val_f1_metric did not improve from 0.99195\n",
      "\n",
      "Epoch 00017: val_f1_metric did not improve from 0.99195\n",
      "***[RESULT]*** ACT Accuracy: 92.17999999999999\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[94.5  1.1  4.4  0. ]\n",
      " [ 2.4 95.1  2.5  0. ]\n",
      " [11.5  1.3 87.1  0. ]\n",
      " [ 0.6  0.1  0.  99.3]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 0.9229514471239573\n"
     ]
    }
   ],
   "source": [
    "#*******************\n",
    "sample_rate = 15 #Hz\n",
    "#*******************\n",
    "num_sampels = (128*sample_rate)//50\n",
    "print(\"[INFO] -- For Sample Rate: \"+str(sample_rate)+\" --> Number of Samples: \"+str(num_sampels))\n",
    "results[sample_rate] = eval_act_id(sdt, mode, ep, cga, num_sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- For Sample Rate: 10 --> Number of Samples: 25\n",
      "[INFO] -- Training Sections: (50532, 2, 25)\n",
      "[INFO] -- Validation Sections: (11288, 2, 25)\n",
      "[INFO] -- Test Sections: (11589, 2, 25)\n",
      "[INFO] -- Training Sections: (50532, 2, 25, 1)\n",
      "[INFO] -- Validation Sections: (11288, 2, 25, 1)\n",
      "[INFO] -- Test Sections: (11589, 2, 25, 1)\n",
      "___________________________________________________\n",
      "Model Size = 50884\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.96525, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.96525 to 0.98249, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00003: val_f1_metric improved from 0.98249 to 0.98367, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00004: val_f1_metric did not improve from 0.98367\n",
      "\n",
      "Epoch 00005: val_f1_metric did not improve from 0.98367\n",
      "\n",
      "Epoch 00006: val_f1_metric did not improve from 0.98367\n",
      "\n",
      "Epoch 00007: val_f1_metric did not improve from 0.98367\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.98367\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.98367\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.98367\n",
      "***[RESULT]*** ACT Accuracy: 91.59\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[89.2  1.9  9.   0. ]\n",
      " [ 4.3 87.3  8.3  0. ]\n",
      " [ 1.6  6.9 91.5  0. ]\n",
      " [ 1.4  0.   0.  98.6]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 0.9150672983746342\n"
     ]
    }
   ],
   "source": [
    "#*******************\n",
    "sample_rate = 10 #Hz\n",
    "#*******************\n",
    "num_sampels = (128*sample_rate)//50\n",
    "print(\"[INFO] -- For Sample Rate: \"+str(sample_rate)+\" --> Number of Samples: \"+str(num_sampels))\n",
    "results[sample_rate] = eval_act_id(sdt, mode, ep, cga, num_sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- For Sample Rate: 5 --> Number of Samples: 12\n",
      "[INFO] -- Training Sections: (50532, 2, 12)\n",
      "[INFO] -- Validation Sections: (11288, 2, 12)\n",
      "[INFO] -- Test Sections: (11589, 2, 12)\n",
      "[INFO] -- Training Sections: (50532, 2, 12, 1)\n",
      "[INFO] -- Validation Sections: (11288, 2, 12, 1)\n",
      "[INFO] -- Test Sections: (11589, 2, 12, 1)\n",
      "___________________________________________________\n",
      "Model Size = 34500\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.89679, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.89679 to 0.93336, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00003: val_f1_metric improved from 0.93336 to 0.94338, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00004: val_f1_metric improved from 0.94338 to 0.94922, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00005: val_f1_metric improved from 0.94922 to 0.95825, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00006: val_f1_metric improved from 0.95825 to 0.96363, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00007: val_f1_metric improved from 0.96363 to 0.96457, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.96457\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.96457\n",
      "\n",
      "Epoch 00010: val_f1_metric improved from 0.96457 to 0.96758, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.96758\n",
      "\n",
      "Epoch 00012: val_f1_metric improved from 0.96758 to 0.96875, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.96875\n",
      "\n",
      "Epoch 00014: val_f1_metric improved from 0.96875 to 0.97187, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00015: val_f1_metric did not improve from 0.97187\n",
      "\n",
      "Epoch 00016: val_f1_metric did not improve from 0.97187\n",
      "\n",
      "Epoch 00017: val_f1_metric improved from 0.97187 to 0.97430, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00018: val_f1_metric did not improve from 0.97430\n",
      "\n",
      "Epoch 00019: val_f1_metric did not improve from 0.97430\n",
      "\n",
      "Epoch 00020: val_f1_metric did not improve from 0.97430\n",
      "\n",
      "Epoch 00021: val_f1_metric did not improve from 0.97430\n",
      "\n",
      "Epoch 00022: val_f1_metric did not improve from 0.97430\n",
      "\n",
      "Epoch 00023: val_f1_metric did not improve from 0.97430\n",
      "\n",
      "Epoch 00024: val_f1_metric did not improve from 0.97430\n",
      "***[RESULT]*** ACT Accuracy: 89.14\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[95.8  1.7  2.2  0.3]\n",
      " [ 7.3 90.7  2.   0. ]\n",
      " [14.3  3.5 82.2  0. ]\n",
      " [ 1.2  0.   0.1 98.7]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 0.8930416441051504\n"
     ]
    }
   ],
   "source": [
    "#*******************\n",
    "sample_rate = 5 #Hz\n",
    "#*******************\n",
    "num_sampels = (128*sample_rate)//50\n",
    "print(\"[INFO] -- For Sample Rate: \"+str(sample_rate)+\" --> Number of Samples: \"+str(num_sampels))\n",
    "results[sample_rate] = eval_act_id(sdt, mode, ep, cga, num_sampels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
