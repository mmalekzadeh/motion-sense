{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "######################################################################################################################\n",
    "import sys\n",
    "import collections\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "\n",
    "plt.style.use('bmh')\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    from IPython.display import clear_output\n",
    "    have_ipython = True\n",
    "except ImportError:\n",
    "    have_ipython = False\n",
    "\n",
    "class KnnDtw(object):\n",
    "    \"\"\"K-nearest neighbor classifier using dynamic time warping\n",
    "    as the distance measure between pairs of time series arrays\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    n_neighbors : int, optional (default = 5)\n",
    "        Number of neighbors to use by default for KNN\n",
    "        \n",
    "    max_warping_window : int, optional (default = infinity)\n",
    "        Maximum warping window allowed by the DTW dynamic\n",
    "        programming function\n",
    "            \n",
    "    subsample_step : int, optional (default = 1)\n",
    "        Step size for the timeseries array. By setting subsample_step = 2,\n",
    "        the timeseries length will be reduced by 50% because every second\n",
    "        item is skipped. Implemented by x[:, ::subsample_step]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_neighbors=5, max_warping_window=10000, subsample_step=1):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.max_warping_window = max_warping_window\n",
    "        self.subsample_step = subsample_step\n",
    "    \n",
    "    def fit(self, x, l):\n",
    "        \"\"\"Fit the model using x as training data and l as class labels\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        x : array of shape [n_samples, n_timepoints]\n",
    "            Training data set for input into KNN classifer\n",
    "            \n",
    "        l : array of shape [n_samples]\n",
    "            Training labels for input into KNN classifier\n",
    "        \"\"\"\n",
    "        \n",
    "        self.x = x\n",
    "        self.l = l\n",
    "        \n",
    "    def _dtw_distance(self, ts_a, ts_b, d = lambda x,y: abs(x-y)):\n",
    "        \"\"\"Returns the DTW similarity distance between two 2-D\n",
    "        timeseries numpy arrays.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        ts_a, ts_b : array of shape [n_samples, n_timepoints]\n",
    "            Two arrays containing n_samples of timeseries data\n",
    "            whose DTW distance between each sample of A and B\n",
    "            will be compared\n",
    "        \n",
    "        d : DistanceMetric object (default = abs(x-y))\n",
    "            the distance measure used for A_i - B_j in the\n",
    "            DTW dynamic programming function\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        DTW distance between A and B\n",
    "        \"\"\"\n",
    "\n",
    "        # Create cost matrix via broadcasting with large int\n",
    "        ts_a, ts_b = np.array(ts_a), np.array(ts_b)\n",
    "        M, N = len(ts_a), len(ts_b)\n",
    "        cost = sys.maxsize * np.ones((M, N))\n",
    "\n",
    "        # Initialize the first row and column\n",
    "        cost[0, 0] = d(ts_a[0], ts_b[0])\n",
    "        for i in range(1, M):\n",
    "            cost[i, 0] = cost[i-1, 0] + d(ts_a[i], ts_b[0])\n",
    "\n",
    "        for j in range(1, N):\n",
    "            cost[0, j] = cost[0, j-1] + d(ts_a[0], ts_b[j])\n",
    "\n",
    "        # Populate rest of cost matrix within window\n",
    "        for i in range(1, M):\n",
    "            for j in range(max(1, i - self.max_warping_window),\n",
    "                            min(N, i + self.max_warping_window)):\n",
    "                choices = cost[i - 1, j - 1], cost[i, j-1], cost[i-1, j]\n",
    "                cost[i, j] = min(choices) + d(ts_a[i], ts_b[j])\n",
    "\n",
    "        # Return DTW distance given window \n",
    "        return cost[-1, -1]\n",
    "    \n",
    "    def _dist_matrix(self, x, y):\n",
    "        \"\"\"Computes the M x N distance matrix between the training\n",
    "        dataset and testing dataset (y) using the DTW distance measure\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        x : array of shape [n_samples, n_timepoints]\n",
    "        \n",
    "        y : array of shape [n_samples, n_timepoints]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Distance matrix between each item of x and y with\n",
    "            shape [training_n_samples, testing_n_samples]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute the distance matrix        \n",
    "        dm_count = 0\n",
    "        \n",
    "        # Compute condensed distance matrix (upper triangle) of pairwise dtw distances\n",
    "        # when x and y are the same array\n",
    "        if(np.array_equal(x, y)):\n",
    "            x_s = shape(x)\n",
    "            dm = np.zeros((x_s[0] * (x_s[0] - 1)) // 2, dtype=np.double)\n",
    "            \n",
    "            #p = ProgressBar(shape(dm)[0])\n",
    "            \n",
    "            for i in range(0, x_s[0] - 1):\n",
    "                for j in range(i + 1, x_s[0]):\n",
    "                    dm[dm_count] = self._dtw_distance(x[i, ::self.subsample_step],\n",
    "                                                      y[j, ::self.subsample_step])\n",
    "                    \n",
    "                    dm_count += 1\n",
    "                    #p.animate(dm_count)\n",
    "            \n",
    "            # Convert to squareform\n",
    "            dm = squareform(dm)\n",
    "            return dm\n",
    "        \n",
    "        # Compute full distance matrix of dtw distnces between x and y\n",
    "        else:\n",
    "            x_s = np.shape(x)\n",
    "            y_s = np.shape(y)\n",
    "            dm = np.zeros((x_s[0], y_s[0])) \n",
    "            dm_size = x_s[0]*y_s[0]\n",
    "            \n",
    "            #p = ProgressBar(dm_size)\n",
    "        \n",
    "            for i in range(0, x_s[0]):\n",
    "                for j in range(0, y_s[0]):\n",
    "                    dm[i, j] = self._dtw_distance(x[i, ::self.subsample_step],\n",
    "                                                  y[j, ::self.subsample_step])\n",
    "                    # Update progress bar\n",
    "                    dm_count += 1\n",
    "                    #p.animate(dm_count)\n",
    "            return dm\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the class labels or probability estimates for \n",
    "        the provided data\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "          x : array of shape [n_samples, n_timepoints]\n",
    "              Array containing the testing data set to be classified\n",
    "          \n",
    "        Returns\n",
    "        -------\n",
    "          2 arrays representing:\n",
    "              (1) the predicted class labels \n",
    "              (2) the knn label count probability\n",
    "        \"\"\"\n",
    "        \n",
    "        dm = self._dist_matrix(x, self.x)\n",
    "\n",
    "        # Identify the k nearest neighbors\n",
    "        knn_idx = dm.argsort()[:, :self.n_neighbors]\n",
    "\n",
    "        # Identify k nearest labels\n",
    "        knn_labels = self.l[knn_idx]\n",
    "        \n",
    "        # Model Label\n",
    "        mode_data = mode(knn_labels, axis=1)\n",
    "        mode_label = mode_data[0]\n",
    "        mode_proba = mode_data[1]/self.n_neighbors\n",
    "\n",
    "        return mode_label.ravel(), mode_proba.ravel()\n",
    "\n",
    "class ProgressBar:\n",
    "    \"\"\"This progress bar was taken from PYMC\n",
    "    \"\"\"\n",
    "    def __init__(self, iterations):\n",
    "        self.iterations = iterations\n",
    "        self.prog_bar = '[]'\n",
    "        self.fill_char = '*'\n",
    "        self.width = 40\n",
    "        self.__update_amount(0)\n",
    "        if have_ipython:\n",
    "            self.animate = self.animate_ipython\n",
    "        else:\n",
    "            self.animate = self.animate_noipython\n",
    "\n",
    "    def animate_ipython(self, iter):\n",
    "        sys.stdout.write('\\r%s'%self)\n",
    "        sys.stdout.flush()\n",
    "        self.update_iteration(iter + 1)\n",
    "\n",
    "    def update_iteration(self, elapsed_iter):\n",
    "        self.__update_amount((elapsed_iter / float(self.iterations)) * 100.0)\n",
    "        self.prog_bar += '  %d of %s complete' % (elapsed_iter, self.iterations)\n",
    "\n",
    "    def __update_amount(self, new_amount):\n",
    "        percent_done = int(round((new_amount / 100.0) * 100.0))\n",
    "        all_full = self.width - 2\n",
    "        num_hashes = int(round((percent_done / 100.0) * all_full))\n",
    "        self.prog_bar = '[' + self.fill_char * num_hashes + ' ' * (all_full - num_hashes) + ']'\n",
    "        pct_place = (len(self.prog_bar) // 2) - len(str(percent_done))\n",
    "        pct_string = '%d%%' % percent_done\n",
    "        self.prog_bar = self.prog_bar[0:pct_place] + \\\n",
    "            (pct_string + self.prog_bar[pct_place + len(pct_string):])\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.prog_bar)\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "\n",
    "import tensorflow as tf \n",
    "import keras \n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from collections import Counter\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model, load_model, model_from_json \n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, Concatenate,  Dropout \n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from scipy.signal import resample\n",
    "def get_class_weights(y):\n",
    "    counter = Counter(y)\n",
    "    majority = max(counter.values())\n",
    "    return  {cls: float(majority/count) for cls, count in counter.items()}\n",
    "\n",
    "\n",
    "\n",
    "class Estimator:\n",
    "    l2p = 0.001\n",
    "    @staticmethod\n",
    "    def early_layers(inp, fm = (1,3), hid_act_func=\"relu\"):\n",
    "        # Start\n",
    "        x = Conv2D(64, fm, padding=\"same\", kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(inp)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(1, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        \n",
    "        # 1\n",
    "        x = Conv2D(64, fm, padding=\"same\", kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(1, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def late_layers(inp, num_classes, fm = (1,3), act_func=\"softmax\", hid_act_func=\"relu\", b_name=\"Identifier\"):\n",
    "        # 2\n",
    "        x = Conv2D(32, fm, padding=\"same\", kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(inp)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(1, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        \n",
    "        # 3\n",
    "        x = Conv2D(32, fm, padding=\"same\", kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(1, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        \n",
    "        # End\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(256, kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(64, kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(num_classes, activation=act_func, name = b_name)(x)\n",
    "\n",
    "        return x\n",
    "   \n",
    "    @staticmethod\n",
    "    def build(height, width, num_classes, name, fm = (1,3), act_func=\"softmax\",hid_act_func=\"relu\"):\n",
    "        inp = Input(shape=(height, width, 1))\n",
    "        early = Estimator.early_layers(inp, fm, hid_act_func=hid_act_func)\n",
    "        late  = Estimator.late_layers(early, num_classes, fm, act_func=act_func, hid_act_func=hid_act_func)\n",
    "        model = Model(inputs=inp, outputs=late ,name=name)\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "def get_ds_infos():\n",
    "    \"\"\"\n",
    "    Read the file includes data subject information.\n",
    "    \n",
    "    Data Columns:\n",
    "    0: code [1-24]\n",
    "    1: weight [kg]\n",
    "    2: height [cm]\n",
    "    3: age [years]\n",
    "    4: gender [0:Female, 1:Male]\n",
    "    \n",
    "    Returns:\n",
    "        A pandas DataFrame that contains inforamtion about data subjects' attributes \n",
    "    \"\"\" \n",
    "\n",
    "    dss = pd.read_csv(\"data_subjects_info.csv\")\n",
    "    print(\"[INFO] -- Data subjects' information is imported.\")\n",
    "    \n",
    "    return dss\n",
    "\n",
    "def set_data_types(data_types=[\"userAcceleration\"]):\n",
    "    \"\"\"\n",
    "    Select the sensors and the mode to shape the final dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_types: A list of sensor data type from this list: [attitude, gravity, rotationRate, userAcceleration] \n",
    "\n",
    "    Returns:\n",
    "        It returns a list of columns to use for creating time-series from files.\n",
    "    \"\"\"\n",
    "    dt_list = []\n",
    "    for t in data_types:\n",
    "        if t != \"attitude\":\n",
    "            dt_list.append([t+\".x\",t+\".y\",t+\".z\"])\n",
    "        else:\n",
    "            dt_list.append([t+\".roll\", t+\".pitch\", t+\".yaw\"])\n",
    "\n",
    "    return dt_list\n",
    "\n",
    "\n",
    "def creat_time_series(dt_list, act_labels, trial_codes, mode=\"mag\", labeled=True, combine_grav_acc=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dt_list: A list of columns that shows the type of data we want.\n",
    "        act_labels: list of activites\n",
    "        trial_codes: list of trials\n",
    "        mode: It can be \"raw\" which means you want raw data\n",
    "        for every dimention of each data type,\n",
    "        [attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)].\n",
    "        or it can be \"mag\" which means you only want the magnitude for each data type: (x^2+y^2+z^2)^(1/2)\n",
    "        labeled: True, if we want a labeld dataset. False, if we only want sensor values.\n",
    "        combine_grav_acc: True, means adding each axis of gravity to  corresponding axis of userAcceleration.\n",
    "    Returns: \n",
    "        It returns a time-series of sensor data.\n",
    "    \n",
    "    \"\"\"\n",
    "    num_data_cols = len(dt_list) if mode == \"mag\" else len(dt_list*3)\n",
    "\n",
    "    if labeled:\n",
    "        dataset = np.zeros((0,num_data_cols+7)) # \"7\" --> [act, code, weight, height, age, gender, trial] \n",
    "    else:\n",
    "        dataset = np.zeros((0,num_data_cols))\n",
    "        \n",
    "    ds_list = get_ds_infos()\n",
    "    \n",
    "    print(\"[INFO] -- Creating Time-Series\")\n",
    "    for sub_id in ds_list[\"code\"]:\n",
    "        for act_id, act in enumerate(act_labels):\n",
    "            for trial in trial_codes[act_id]:\n",
    "                fname = 'A_DeviceMotion_data/'+act+'_'+str(trial)+'/sub_'+str(int(sub_id))+'.csv'\n",
    "                raw_data = pd.read_csv(fname)\n",
    "                raw_data = raw_data.drop(['Unnamed: 0'], axis=1)\n",
    "                vals = np.zeros((len(raw_data), num_data_cols))\n",
    "                \n",
    "                if combine_grav_acc:\n",
    "                    raw_data[\"userAcceleration.x\"] = raw_data[\"userAcceleration.x\"].add(raw_data[\"gravity.x\"])\n",
    "                    raw_data[\"userAcceleration.y\"] = raw_data[\"userAcceleration.y\"].add(raw_data[\"gravity.y\"])\n",
    "                    raw_data[\"userAcceleration.z\"] = raw_data[\"userAcceleration.z\"].add(raw_data[\"gravity.z\"])\n",
    "                \n",
    "                for x_id, axes in enumerate(dt_list):\n",
    "                    if mode == \"mag\":\n",
    "                        vals[:,x_id] = (raw_data[axes]**2).sum(axis=1)**0.5        \n",
    "                    else:\n",
    "                        vals[:,x_id*3:(x_id+1)*3] = raw_data[axes].values\n",
    "                    vals = vals[:,:num_data_cols]\n",
    "                if labeled:\n",
    "                    lbls = np.array([[act_id,\n",
    "                            sub_id-1,\n",
    "                            ds_list[\"weight\"][sub_id-1],\n",
    "                            ds_list[\"height\"][sub_id-1],\n",
    "                            ds_list[\"age\"][sub_id-1],\n",
    "                            ds_list[\"gender\"][sub_id-1],\n",
    "                            trial          \n",
    "                           ]]*len(raw_data))\n",
    "                    vals = np.concatenate((vals, lbls), axis=1)\n",
    "                dataset = np.append(dataset,vals, axis=0)\n",
    "    cols = []\n",
    "    for axes in dt_list:\n",
    "        if mode == \"raw\":\n",
    "            cols += axes\n",
    "        else:\n",
    "            cols += [str(axes[0][:-2])]\n",
    "            \n",
    "    if labeled:\n",
    "        cols += [\"act\", \"id\", \"weight\", \"height\", \"age\", \"gender\", \"trial\"]\n",
    "    \n",
    "    dataset = pd.DataFrame(data=dataset, columns=cols)\n",
    "    return dataset\n",
    "#________________________________\n",
    "#________________________________\n",
    "\n",
    "def ts_to_secs(dataset, w, s, standardize = False, **options):\n",
    "    \n",
    "    data = dataset[dataset.columns[:-7]].values    \n",
    "    act_labels = dataset[\"act\"].values\n",
    "    id_labels = dataset[\"id\"].values\n",
    "    trial_labels = dataset[\"trial\"].values\n",
    "\n",
    "    mean = 0\n",
    "    std = 1\n",
    "    if standardize:\n",
    "        ## Standardize each sensorâ€™s data to have a zero mean and unity standard deviation.\n",
    "        ## As usual, we normalize test dataset by training dataset's parameters \n",
    "        if options:\n",
    "            mean = options.get(\"mean\")\n",
    "            std = options.get(\"std\")\n",
    "            print(\"[INFO] -- Test Data has been standardized\")\n",
    "        else:\n",
    "            mean = data.mean(axis=0)\n",
    "            std = data.std(axis=0)\n",
    "            print(\"[INFO] -- Training Data has been standardized: the mean is = \"+str(mean)+\" ; and the std is = \"+str(std))            \n",
    "\n",
    "        data -= mean\n",
    "        data /= std\n",
    "    else:\n",
    "        print(\"[INFO] -- Without Standardization.....\")\n",
    "\n",
    "    ## We want the Rows of matrices show each Feature and the Columns show time points.\n",
    "    data = data.T\n",
    "\n",
    "    m = data.shape[0]   # Data Dimension \n",
    "    ttp = data.shape[1] # Total Time Points\n",
    "    number_of_secs = int(round(((ttp - w)/s)))\n",
    "\n",
    "    ##  Create a 3D matrix for Storing Sections  \n",
    "    secs_data = np.zeros((number_of_secs , m , w ))\n",
    "    act_secs_labels = np.zeros(number_of_secs)\n",
    "    id_secs_labels = np.zeros(number_of_secs)\n",
    "\n",
    "    k=0\n",
    "    for i in range(0 , ttp-w, s):\n",
    "        j = i // s\n",
    "        if j >= number_of_secs:\n",
    "            break\n",
    "        if id_labels[i] != id_labels[i+w-1]: \n",
    "            continue\n",
    "        if act_labels[i] != act_labels[i+w-1]: \n",
    "            continue\n",
    "        if trial_labels[i] != trial_labels[i+w-1]:\n",
    "            continue\n",
    "            \n",
    "        secs_data[k] = data[:, i:i+w]\n",
    "        act_secs_labels[k] = act_labels[i].astype(int)\n",
    "        id_secs_labels[k] = id_labels[i].astype(int)\n",
    "        k = k+1\n",
    "        \n",
    "    secs_data = secs_data[0:k]\n",
    "    act_secs_labels = act_secs_labels[0:k]\n",
    "    id_secs_labels = id_secs_labels[0:k]\n",
    "    return secs_data, act_secs_labels, id_secs_labels, mean, std\n",
    "##________________________________________________________________\n",
    "\n",
    "\n",
    "ACT_LABELS = [\"dws\",\"ups\", \"wlk\", \"jog\", \"std\", \"sit\"]\n",
    "TRIAL_CODES = {\n",
    "    ACT_LABELS[0]:[1,2,11],\n",
    "    ACT_LABELS[1]:[3,4,12],\n",
    "    ACT_LABELS[2]:[7,8,15],\n",
    "    ACT_LABELS[3]:[9,16],\n",
    "    ACT_LABELS[4]:[6,14],\n",
    "    ACT_LABELS[5]:[5,13],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- Selected sensor data types: ['rotationRate', 'userAcceleration'] -- Mode: mag -- Grav+Acc: True\n",
      "[INFO] -- Selected activites: ['dws', 'ups', 'wlk', 'jog']\n",
      "[INFO] -- Data subjects' information is imported.\n",
      "[INFO] -- Creating Time-Series\n",
      "[INFO] -- Shape of time-Series dataset:(767660, 9)\n",
      "[INFO] -- Test IDs: [4, 9, 11, 21]\n",
      "[INFO] -- Shape of Train Time-Series :(646207, 9)\n",
      "[INFO] -- Shape of Test Time-Series :(121453, 9)\n"
     ]
    }
   ],
   "source": [
    "## Here we set parameter to build labeld time-series from dataset of \"(A)DeviceMotion_data\"\n",
    "## attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)\n",
    "sdt = [\"rotationRate\",\"userAcceleration\"]\n",
    "mode = \"mag\"\n",
    "cga = True # Add gravity to acceleration or not\n",
    "print(\"[INFO] -- Selected sensor data types: \"+str(sdt)+\" -- Mode: \"+str(mode)+\" -- Grav+Acc: \"+str(cga))    \n",
    "\n",
    "act_labels = ACT_LABELS [0:4]\n",
    "print(\"[INFO] -- Selected activites: \"+str(act_labels))    \n",
    "trial_codes = [TRIAL_CODES[act] for act in act_labels]\n",
    "dt_list = set_data_types(sdt)\n",
    "dataset = creat_time_series(dt_list, act_labels, trial_codes, mode=mode, labeled=True, combine_grav_acc = cga)\n",
    "print(\"[INFO] -- Shape of time-Series dataset:\"+str(dataset.shape))    \n",
    "\n",
    "#*****************\n",
    "TRAIN_TEST_TYPE = \"subject\" # \"subject\" or \"trial\"\n",
    "#*****************\n",
    "\n",
    "if TRAIN_TEST_TYPE == \"subject\":\n",
    "    test_ids = [4,9,11,21]\n",
    "    print(\"[INFO] -- Test IDs: \"+str(test_ids))\n",
    "    test_ts = dataset.loc[(dataset['id'].isin(test_ids))]\n",
    "    train_ts = dataset.loc[~(dataset['id'].isin(test_ids))]\n",
    "else:\n",
    "    test_trail = [11,12,13,14,15,16]  \n",
    "    print(\"[INFO] -- Test Trials: \"+str(test_trail))\n",
    "    test_ts = dataset.loc[(dataset['trial'].isin(test_trail))]\n",
    "    train_ts = dataset.loc[~(dataset['trial'].isin(test_trail))]\n",
    "\n",
    "    \n",
    "print(\"[INFO] -- Shape of Train Time-Series :\"+str(train_ts.shape))\n",
    "print(\"[INFO] -- Shape of Test Time-Series :\"+str(test_ts.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- Shape of Train Time-Series :(523129, 9)\n",
      "[INFO] -- Shape of Test Time-Series :(123078, 9)\n"
     ]
    }
   ],
   "source": [
    "val_trail = [11,12,13,14,15,16]\n",
    "val_ts = train_ts.loc[(train_ts['trial'].isin(val_trail))]\n",
    "train_ts = train_ts.loc[~(train_ts['trial'].isin(val_trail))]\n",
    "print(\"[INFO] -- Shape of Train Time-Series :\"+str(train_ts.shape))\n",
    "print(\"[INFO] -- Shape of Test Time-Series :\"+str(val_ts.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- Training Data has been standardized: the mean is = [2.17728825 1.19431016] ; and the std is = [1.43229632 0.70168121]\n",
      "[INFO] -- Test Data has been standardized\n",
      "[INFO] -- Test Data has been standardized\n",
      "[INFO] -- Shape of Training Sections: (50532, 2, 128)\n",
      "[INFO] -- Shape of Training Sections: (11288, 2, 128)\n",
      "[INFO] -- Shape of Test Sections:  (11589, 2, 128)\n"
     ]
    }
   ],
   "source": [
    "#************\n",
    "## HERE ##\n",
    "\n",
    "## This Variable Defines the Size of Sliding Window\n",
    "## ( e.g. 100 means in each snapshot we just consider 100 consecutive observations of each sensor) \n",
    "w = 128 # 50 Equals to 1 second for MotionSense Dataset (it is on 50Hz samplig rate)\n",
    "## Here We Choose Step Size for Building Diffrent Snapshots from Time-Series Data\n",
    "## ( smaller step size will increase the amount of the instances and higher computational cost may be incurred )\n",
    "s = 10\n",
    "train_data, act_train, id_train, train_mean, train_std = ts_to_secs(train_ts.copy(),\n",
    "                                                                   w,\n",
    "                                                                   s,\n",
    "                                                                   standardize = True)\n",
    "\n",
    "s = 10\n",
    "val_data, act_val, id_val, val_mean, val_std = ts_to_secs(val_ts.copy(),\n",
    "                                                              w,\n",
    "                                                              s,\n",
    "                                                              standardize = True,\n",
    "                                                              mean = train_mean, \n",
    "                                                              std = train_std)\n",
    "\n",
    "\n",
    "s = 10\n",
    "test_data, act_test, id_test, test_mean, test_std = ts_to_secs(test_ts.copy(),\n",
    "                                                              w,\n",
    "                                                              s,\n",
    "                                                              standardize = True,\n",
    "                                                              mean = train_mean, \n",
    "                                                              std = train_std)\n",
    "\n",
    "print(\"[INFO] -- Shape of Training Sections: \"+str(train_data.shape))\n",
    "print(\"[INFO] -- Shape of Training Sections: \"+str(val_data.shape))\n",
    "print(\"[INFO] -- Shape of Test Sections:  \"+str(test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- Shape of Training Sections: (50532, 2, 128, 1)\n",
      "[INFO] -- Validation Sections:(11288, 2, 128, 1)\n",
      "[INFO] -- Shape of Training Sections: (11589, 2, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "id_train_labels = to_categorical(id_train)\n",
    "id_val_labels = to_categorical(id_val)\n",
    "id_test_labels = to_categorical(id_test)\n",
    "id_test_labels = np.append(id_test_labels, np.zeros((len(id_test_labels),2)), axis =1)\n",
    "\n",
    "act_train_labels = to_categorical(act_train)\n",
    "act_val_labels = to_categorical(act_val)\n",
    "act_test_labels = to_categorical(act_test)\n",
    "    \n",
    "## Here we add an extra dimension to the datasets just to be ready for using with Convolution2D\n",
    "train_data = np.expand_dims(train_data,axis=3)\n",
    "print(\"[INFO] -- Shape of Training Sections:\", train_data.shape)\n",
    "val_data = np.expand_dims(val_data,axis=3)\n",
    "print(\"[INFO] -- Validation Sections:\"+str(val_data.shape))\n",
    "test_data = np.expand_dims(test_data,axis=3)\n",
    "print(\"[INFO] -- Shape of Training Sections:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/45305384/5210098\n",
    "def f1_metric(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_act(X,Yact, vX, vYact, tX, tYact, ep=50):\n",
    "    height = X.shape[1]\n",
    "    width = X.shape[2]\n",
    "    act_class_numbers = 4\n",
    "    fm = (2,5)\n",
    "\n",
    "    ## Callbacks\n",
    "    eval_metric= \"val_f1_metric\"    \n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor=eval_metric, mode='max', patience = 10)\n",
    "    filepath=\"RAWACT.best.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor=eval_metric, verbose=0, save_best_only=True, mode='max')\n",
    "    callbacks_list = [early_stop,checkpoint]\n",
    "    ## Callbacks\n",
    "    eval_act = Estimator.build(height, width, act_class_numbers, name =\"EVAL_ACT\", fm=fm, act_func=\"softmax\",hid_act_func=\"relu\")\n",
    "    eval_act.compile( loss=\"categorical_crossentropy\", optimizer='adam', metrics=['acc', f1_metric])\n",
    "\n",
    "    eval_act.fit(X, Yact,\n",
    "                validation_data = (vX, vYact),\n",
    "                epochs = ep,\n",
    "                batch_size = 128,\n",
    "                verbose = 0,\n",
    "                class_weight = get_class_weights(np.argmax(Yact,axis=1)),\n",
    "                callbacks = callbacks_list\n",
    "               )\n",
    "\n",
    "    eval_act.load_weights(\"RAWACT.best.hdf5\")\n",
    "    eval_act.compile( loss=\"categorical_crossentropy\", optimizer='adam', metrics=['acc',f1_metric])\n",
    "\n",
    "    result1 = eval_act.evaluate(tX, tYact, verbose = 2)\n",
    "    act_acc = result1[1].round(4)*100\n",
    "    print(\"***[RESULT]*** ACT Accuracy: \"+str(act_acc))\n",
    "\n",
    "    preds = eval_act.predict(tX)\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    conf_mat = confusion_matrix(np.argmax(tYact, axis=1), preds)\n",
    "    conf_mat = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "    print(\"***[RESULT]*** ACT  Confusion Matrix\")\n",
    "    print(np.array(conf_mat).round(3)*100)  \n",
    "\n",
    "    f1act = f1_score(np.argmax(tYact, axis=1), preds, average=None).mean()\n",
    "    print(\"***[RESULT]*** ACT Averaged F-1 Score : \"+str(f1act*100))\n",
    "    return f1act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "Iteration 0\n",
      "***[RESULT]*** ACT Accuracy: 95.65\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[94.4  1.4  3.9  0.3]\n",
      " [ 2.1 96.1  1.9  0. ]\n",
      " [ 5.   0.5 94.5  0. ]\n",
      " [ 0.8  0.1  0.  99.2]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 95.39048921491855\n",
      "##################\n",
      "##################\n",
      "Iteration 1\n",
      "***[RESULT]*** ACT Accuracy: 91.16\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[95.9  1.7  2.4  0. ]\n",
      " [ 0.8 97.2  2.   0. ]\n",
      " [15.7  0.5 83.8  0. ]\n",
      " [ 1.1  0.3  0.  98.5]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 91.51869910269377\n",
      "##################\n",
      "##################\n",
      "Iteration 2\n",
      "***[RESULT]*** ACT Accuracy: 94.05\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[92.8  1.   5.8  0.4]\n",
      " [ 1.4 95.4  3.2  0. ]\n",
      " [ 7.7  0.5 91.7  0.1]\n",
      " [ 0.4  0.   0.  99.6]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 93.96873409170237\n",
      "##################\n",
      "##################\n",
      "Iteration 3\n",
      "***[RESULT]*** ACT Accuracy: 89.53\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[91.7  2.3  5.7  0.3]\n",
      " [ 2.  96.5  1.5  0. ]\n",
      " [13.3  2.4 84.3  0. ]\n",
      " [ 0.2  7.1  0.  92.7]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 89.36982893546788\n",
      "##################\n",
      "##################\n",
      "Iteration 4\n",
      "***[RESULT]*** ACT Accuracy: 92.28\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[92.9  1.2  5.   0.9]\n",
      " [ 2.1 94.3  3.6  0. ]\n",
      " [11.   0.9 88.1  0. ]\n",
      " [ 0.1  0.1  0.  99.8]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 92.31094261399153\n",
      "##################\n",
      "Raw Data, F1 Scores: [0.95390489 0.91518699 0.93968734 0.89369829 0.92310943]\n",
      "Mean: 0.9251173879175483\n",
      "STD: 0.020643683671778375\n"
     ]
    }
   ],
   "source": [
    "X = train_data.copy()\n",
    "Yact = act_train_labels\n",
    "vX = val_data.copy()\n",
    "vYact = act_val_labels\n",
    "tX = test_data.copy()\n",
    "tYact = act_test_labels\n",
    "ep=50\n",
    "raw_f1 = np.zeros(5)\n",
    "for i in range(5):\n",
    "    print(\"##################\")\n",
    "    print(\"Iteration \"+str(i))\n",
    "    raw_f1[i] = eval_act(X, Yact, vX, vYact, tX, tYact, ep)\n",
    "    print(\"##################\")\n",
    "print(\"Raw Data, F1 Scores: \"+str(raw_f1))\n",
    "print(\"Mean: \"+str(raw_f1.mean()))\n",
    "print(\"STD: \"+str(raw_f1.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "50532/50532 [==============================] - 20s 397us/step\n",
      "11288/11288 [==============================] - 4s 393us/step\n",
      "11589/11589 [==============================] - 5s 389us/step\n",
      "##################\n",
      "Iteration 0\n",
      "***[RESULT]*** ACT Accuracy: 93.16\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[87.7  4.7  5.8  1.9]\n",
      " [ 4.8 92.5  2.8  0. ]\n",
      " [ 2.4  4.5 93.1  0. ]\n",
      " [ 0.6  0.5  0.  99. ]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 92.559842680656\n",
      "##################\n",
      "##################\n",
      "Iteration 1\n",
      "***[RESULT]*** ACT Accuracy: 94.24\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[86.   3.6  9.9  0.5]\n",
      " [ 4.2 90.3  5.5  0. ]\n",
      " [ 1.4  1.6 97.1  0. ]\n",
      " [ 0.2  0.8  0.  99. ]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 93.62119596524798\n",
      "##################\n",
      "##################\n",
      "Iteration 2\n",
      "***[RESULT]*** ACT Accuracy: 93.63\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[92.6  1.6  5.3  0.4]\n",
      " [ 9.3 86.2  4.5  0. ]\n",
      " [ 2.3  2.3 95.4  0. ]\n",
      " [ 0.3  1.2  0.  98.5]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 92.88325251714515\n",
      "##################\n",
      "##################\n",
      "Iteration 3\n",
      "***[RESULT]*** ACT Accuracy: 92.97\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[88.6  5.   6.3  0.1]\n",
      " [ 3.7 93.8  2.5  0. ]\n",
      " [ 2.1  5.8 92.1  0. ]\n",
      " [ 0.3  1.7  0.  98. ]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 92.62698230276779\n",
      "##################\n",
      "##################\n",
      "Iteration 4\n",
      "***[RESULT]*** ACT Accuracy: 93.67999999999999\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[88.6  2.4  8.9  0.1]\n",
      " [ 8.5 87.8  3.7  0. ]\n",
      " [ 2.1  1.8 96.1  0. ]\n",
      " [ 0.5  0.7  0.  98.8]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 92.88018428655526\n",
      "##################\n",
      "CAE Data, F1 Scores: [0.92559843 0.93621196 0.92883253 0.92626982 0.92880184]\n",
      "Mean: 0.9291429155047443\n",
      "STD: 0.003768344951836892\n"
     ]
    }
   ],
   "source": [
    "lm_file = \"msda_anon_model\"\n",
    "\n",
    "json_file = open(lm_file+\".json\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "anon_model = model_from_json(loaded_model_json)\n",
    "anon_model.load_weights(lm_file+\"_weights.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "\n",
    "X = anon_model.predict(train_data, verbose=1)[0]\n",
    "vX = anon_model.predict(val_data, verbose=1)[0]\n",
    "tX = anon_model.predict(test_data, verbose=1)[0]\n",
    "\n",
    "cae_f1 = np.zeros(5)\n",
    "for i in range(5):\n",
    "    print(\"##################\")\n",
    "    print(\"Iteration \"+str(i))\n",
    "    cae_f1[i] = eval_act(X, Yact, vX, vYact, tX, tYact, ep)\n",
    "    print(\"##################\")\n",
    "print(\"CAE Data, F1 Scores: \"+str(cae_f1))\n",
    "print(\"Mean: \"+str(cae_f1.mean()))\n",
    "print(\"STD: \"+str(cae_f1.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sampels = 25\n",
      "[INFO] -- Training Sections: (50532, 2, 25, 1)\n",
      "[INFO] -- Validation Sections: (11288, 2, 25, 1)\n",
      "[INFO] -- Test Sections: (11589, 2, 25, 1)\n",
      "##################\n",
      "Iteration 0\n",
      "***[RESULT]*** ACT Accuracy: 90.55\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[91.4  3.2  5.4  0. ]\n",
      " [ 2.2 95.8  2.   0. ]\n",
      " [ 1.1 13.8 85.1  0. ]\n",
      " [ 2.6  0.   0.  97.4]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 91.06964573672886\n",
      "##################\n",
      "##################\n",
      "Iteration 1\n",
      "***[RESULT]*** ACT Accuracy: 89.66\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[94.   2.3  3.7  0. ]\n",
      " [ 3.4 93.7  2.9  0. ]\n",
      " [ 5.7 11.6 82.7  0. ]\n",
      " [ 1.5  0.   0.  98.5]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 90.08605965273497\n",
      "##################\n",
      "##################\n",
      "Iteration 2\n",
      "***[RESULT]*** ACT Accuracy: 91.86999999999999\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[89.   2.9  7.9  0.2]\n",
      " [ 2.2 95.4  2.4  0. ]\n",
      " [ 2.1  9.5 88.3  0. ]\n",
      " [ 0.8  0.   0.  99.2]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 92.06199347855019\n",
      "##################\n",
      "##################\n",
      "Iteration 3\n",
      "***[RESULT]*** ACT Accuracy: 90.51\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[96.2  1.   2.5  0.3]\n",
      " [ 3.1 94.6  2.3  0. ]\n",
      " [ 4.7 11.8 83.3  0.2]\n",
      " [ 1.3  0.   0.  98.7]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 90.98387303603229\n",
      "##################\n",
      "##################\n",
      "Iteration 4\n",
      "***[RESULT]*** ACT Accuracy: 90.84\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[92.8  1.3  5.9  0. ]\n",
      " [ 2.4 95.7  1.9  0. ]\n",
      " [ 3.5 12.  84.4  0.1]\n",
      " [ 0.4  0.   0.  99.6]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 91.38218819666952\n",
      "##################\n",
      "Raw Data, F1 Scores: [0.91069646 0.9008606  0.92061993 0.90983873 0.91382188]\n",
      "Mean: 0.9111675202014317\n",
      "STD: 0.006397093654292257\n"
     ]
    }
   ],
   "source": [
    "#*******************\n",
    "sample_rate = 10 #Hz\n",
    "#*******************\n",
    "num_sampels = (128*sample_rate)//50\n",
    "print(\"Number of Sampels = \"+str(num_sampels))\n",
    "X = train_data.copy()\n",
    "vX = val_data.copy()\n",
    "tX = test_data.copy()\n",
    "\n",
    "\n",
    "from scipy.signal import resample\n",
    "ds_train_data = X.copy()\n",
    "ds_val_data = vX.copy()\n",
    "ds_test_data = tX.copy()\n",
    "\n",
    "for sens in range(2):\n",
    "    tmp = np.array([resample(x,num_sampels) for x in ds_train_data[:,sens,:,0]])\n",
    "    ds_train_data[:,sens,:num_sampels,0] = tmp\n",
    "\n",
    "    tmp = np.array([resample(x,num_sampels) for x in ds_val_data[:,sens,:,0]])\n",
    "    ds_val_data[:,sens,:num_sampels,0] = tmp\n",
    "\n",
    "    tmp = np.array([resample(x,num_sampels) for x in ds_test_data[:,sens,:,0]])\n",
    "    ds_test_data[:,sens,:num_sampels,0] = tmp \n",
    "\n",
    "ds_train_data = ds_train_data[:,:,:num_sampels,:]\n",
    "ds_val_data = ds_val_data[:,:,:num_sampels,:]\n",
    "ds_test_data = ds_test_data[:,:,:num_sampels,:]\n",
    "\n",
    "print(\"[INFO] -- Training Sections:\", ds_train_data.shape)\n",
    "print(\"[INFO] -- Validation Sections:\", ds_val_data.shape)\n",
    "print(\"[INFO] -- Test Sections:\", ds_test_data.shape)\n",
    "\n",
    "X = ds_train_data\n",
    "vX = ds_val_data\n",
    "tX = ds_test_data\n",
    "\n",
    "dwnsmpl_f1 = np.zeros(5)\n",
    "for i in range(5):\n",
    "    print(\"##################\")\n",
    "    print(\"Iteration \"+str(i))\n",
    "    dwnsmpl_f1[i] = eval_act(X, Yact, vX, vYact, tX, tYact, ep)\n",
    "    print(\"##################\")\n",
    "print(\"Raw Data, F1 Scores: \"+str(dwnsmpl_f1))\n",
    "print(\"Mean: \"+str(dwnsmpl_f1.mean()))\n",
    "print(\"STD: \"+str(dwnsmpl_f1.std()))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator:\n",
    "    l2p = 0.001\n",
    "    @staticmethod\n",
    "    def early_layers(inp, fm = (1,3), hid_act_func=\"relu\"):\n",
    "        # Start\n",
    "        x = Conv2D(64, fm, padding=\"same\", kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(inp)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(1, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        \n",
    "        # 1\n",
    "        x = Conv2D(64, fm, padding=\"same\", kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(1, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def late_layers(inp, num_classes, fm = (1,3), act_func=\"softmax\", hid_act_func=\"relu\", b_name=\"Identifier\"):\n",
    "        # 2\n",
    "        x = Conv2D(32, fm, padding=\"same\", kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(inp)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(1, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        \n",
    "        # End\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(256, kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(64, kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(num_classes, activation=act_func, name = b_name)(x)\n",
    "\n",
    "        return x\n",
    "   \n",
    "    @staticmethod\n",
    "    def build(height, width, num_classes, name, fm = (1,3), act_func=\"softmax\",hid_act_func=\"relu\"):\n",
    "        inp = Input(shape=(height, width, 1))\n",
    "        early = Estimator.early_layers(inp, fm, hid_act_func=hid_act_func)\n",
    "        late  = Estimator.late_layers(early, num_classes, fm, act_func=act_func, hid_act_func=hid_act_func)\n",
    "        model = Model(inputs=inp, outputs=late ,name=name)\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sampels = 12\n",
      "[INFO] -- Training Sections: (50532, 2, 12, 1)\n",
      "[INFO] -- Validation Sections: (11288, 2, 12, 1)\n",
      "[INFO] -- Test Sections: (11589, 2, 12, 1)\n",
      "##################\n",
      "Iteration 0\n",
      "***[RESULT]*** ACT Accuracy: 90.89\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[94.6  0.8  4.3  0.3]\n",
      " [ 6.7 90.2  3.1  0. ]\n",
      " [10.3  3.  86.7  0. ]\n",
      " [ 1.1  0.1  0.2 98.6]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 90.79879959613612\n",
      "##################\n",
      "##################\n",
      "Iteration 1\n",
      "***[RESULT]*** ACT Accuracy: 85.92\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[95.8  2.1  2.1  0. ]\n",
      " [ 9.4 86.7  3.9  0. ]\n",
      " [17.3  5.6 77.2  0. ]\n",
      " [ 2.   0.1  0.  97.8]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 86.37516629117879\n",
      "##################\n",
      "##################\n",
      "Iteration 2\n",
      "***[RESULT]*** ACT Accuracy: 89.09\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[94.4  3.   2.5  0.1]\n",
      " [ 5.5 91.6  2.8  0. ]\n",
      " [11.8  6.1 82.1  0. ]\n",
      " [ 1.1  0.2  0.1 98.6]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 89.24487189772555\n",
      "##################\n",
      "##################\n",
      "Iteration 3\n",
      "***[RESULT]*** ACT Accuracy: 87.72\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[95.3  1.4  3.2  0.1]\n",
      " [ 9.8 88.2  1.9  0.2]\n",
      " [15.9  3.6 80.5  0. ]\n",
      " [ 1.4  0.   0.2 98.3]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 87.96525965265587\n",
      "##################\n",
      "##################\n",
      "Iteration 4\n",
      "***[RESULT]*** ACT Accuracy: 84.94\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[96.7  1.3  2.1  0. ]\n",
      " [13.2 85.1  1.7  0. ]\n",
      " [22.2  2.6 75.1  0. ]\n",
      " [ 1.3  0.   0.1 98.6]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 85.74168464011052\n",
      "##################\n",
      "Raw Data, F1 Scores: [0.907988   0.86375166 0.89244872 0.8796526  0.85741685]\n",
      "Mean: 0.8802515641556138\n",
      "STD: 0.018504648460658267\n"
     ]
    }
   ],
   "source": [
    "#*******************\n",
    "sample_rate = 5 #Hz\n",
    "#*******************\n",
    "num_sampels = (128*sample_rate)//50\n",
    "print(\"Number of Sampels = \"+str(num_sampels))\n",
    "X = train_data.copy()\n",
    "vX = val_data.copy()\n",
    "tX = test_data.copy()\n",
    "\n",
    "\n",
    "from scipy.signal import resample\n",
    "ds_train_data = X.copy()\n",
    "ds_val_data = vX.copy()\n",
    "ds_test_data = tX.copy()\n",
    "\n",
    "for sens in range(2):\n",
    "    tmp = np.array([resample(x,num_sampels) for x in ds_train_data[:,sens,:,0]])\n",
    "    ds_train_data[:,sens,:num_sampels,0] = tmp\n",
    "\n",
    "    tmp = np.array([resample(x,num_sampels) for x in ds_val_data[:,sens,:,0]])\n",
    "    ds_val_data[:,sens,:num_sampels,0] = tmp\n",
    "\n",
    "    tmp = np.array([resample(x,num_sampels) for x in ds_test_data[:,sens,:,0]])\n",
    "    ds_test_data[:,sens,:num_sampels,0] = tmp \n",
    "\n",
    "ds_train_data = ds_train_data[:,:,:num_sampels,:]\n",
    "ds_val_data = ds_val_data[:,:,:num_sampels,:]\n",
    "ds_test_data = ds_test_data[:,:,:num_sampels,:]\n",
    "\n",
    "print(\"[INFO] -- Training Sections:\", ds_train_data.shape)\n",
    "print(\"[INFO] -- Validation Sections:\", ds_val_data.shape)\n",
    "print(\"[INFO] -- Test Sections:\", ds_test_data.shape)\n",
    "\n",
    "X = ds_train_data\n",
    "vX = ds_val_data\n",
    "tX = ds_test_data\n",
    "\n",
    "dwnsmpl_f1 = np.zeros(5)\n",
    "for i in range(5):\n",
    "    print(\"##################\")\n",
    "    print(\"Iteration \"+str(i))\n",
    "    dwnsmpl_f1[i] = eval_act(X, Yact, vX, vYact, tX, tYact, ep)\n",
    "    print(\"##################\")\n",
    "print(\"Raw Data, F1 Scores: \"+str(dwnsmpl_f1))\n",
    "print(\"Mean: \"+str(dwnsmpl_f1.mean()))\n",
    "print(\"STD: \"+str(dwnsmpl_f1.std()))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSA(object):\n",
    "    \n",
    "    __supported_types = (pd.Series, np.ndarray, list)\n",
    "    \n",
    "    def __init__(self, tseries, L, save_mem=True):\n",
    "        \"\"\"\n",
    "        Decomposes the given time series with a singular-spectrum analysis. Assumes the values of the time series are\n",
    "        recorded at equal intervals.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        tseries : The original time series, in the form of a Pandas Series, NumPy array or list. \n",
    "        L : The window length. Must be an integer 2 <= L <= N/2, where N is the length of the time series.\n",
    "        save_mem : Conserve memory by not retaining the elementary matrices. Recommended for long time series with\n",
    "            thousands of values. Defaults to True.\n",
    "        \n",
    "        Note: Even if an NumPy array or list is used for the initial time series, all time series returned will be\n",
    "        in the form of a Pandas Series or DataFrame object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Tedious type-checking for the initial time series\n",
    "        if not isinstance(tseries, self.__supported_types):\n",
    "            raise TypeError(\"Unsupported time series object. Try Pandas Series, NumPy array or list.\")\n",
    "        \n",
    "        # Checks to save us from ourselves\n",
    "        self.N = len(tseries)\n",
    "        if not 2 <= L <= self.N/2:\n",
    "            raise ValueError(\"The window length must be in the interval [2, N/2].\")\n",
    "        \n",
    "        self.L = L\n",
    "        self.orig_TS = pd.Series(tseries)\n",
    "        self.K = self.N - self.L + 1\n",
    "        \n",
    "        # Embed the time series in a trajectory matrix\n",
    "        self.X = np.array([self.orig_TS.values[i:L+i] for i in range(0, self.K)]).T\n",
    "        \n",
    "        # Decompose the trajectory matrix\n",
    "        self.U, self.Sigma, VT = np.linalg.svd(self.X)\n",
    "        self.d = np.linalg.matrix_rank(self.X)\n",
    "        \n",
    "        self.TS_comps = np.zeros((self.N, self.d))\n",
    "        \n",
    "        if not save_mem:\n",
    "            # Construct and save all the elementary matrices\n",
    "            self.X_elem = np.array([ self.Sigma[i]*np.outer(self.U[:,i], VT[i,:]) for i in range(self.d) ])\n",
    "\n",
    "            # Diagonally average the elementary matrices, store them as columns in array.           \n",
    "            for i in range(self.d):\n",
    "                X_rev = self.X_elem[i, ::-1]\n",
    "                self.TS_comps[:,i] = [X_rev.diagonal(j).mean() for j in range(-X_rev.shape[0]+1, X_rev.shape[1])]\n",
    "            \n",
    "            self.V = VT.T\n",
    "        else:\n",
    "            # Reconstruct the elementary matrices without storing them\n",
    "            for i in range(self.d):\n",
    "                X_elem = self.Sigma[i]*np.outer(self.U[:,i], VT[i,:])\n",
    "                X_rev = X_elem[::-1]\n",
    "                self.TS_comps[:,i] = [X_rev.diagonal(j).mean() for j in range(-X_rev.shape[0]+1, X_rev.shape[1])]\n",
    "            \n",
    "            self.X_elem = \"Re-run with save_mem=False to retain the elementary matrices.\"\n",
    "            \n",
    "            # The V array may also be very large under these circumstances, so we won't keep it.\n",
    "            self.V = \"Re-run with save_mem=False to retain the V matrix.\"\n",
    "        \n",
    "        # Calculate the w-correlation matrix.\n",
    "        self.calc_wcorr()\n",
    "            \n",
    "    def components_to_df(self, n=0):\n",
    "        \"\"\"\n",
    "        Returns all the time series components in a single Pandas DataFrame object.\n",
    "        \"\"\"\n",
    "        if n > 0:\n",
    "            n = min(n, self.d)\n",
    "        else:\n",
    "            n = self.d\n",
    "        \n",
    "        # Create list of columns - call them F0, F1, F2, ...\n",
    "        cols = [\"F{}\".format(i) for i in range(n)]\n",
    "        return pd.DataFrame(self.TS_comps[:, :n], columns=cols, index=self.orig_TS.index)\n",
    "            \n",
    "    \n",
    "    def reconstruct(self, indices):\n",
    "        \"\"\"\n",
    "        Reconstructs the time series from its elementary components, using the given indices. Returns a Pandas Series\n",
    "        object with the reconstructed time series.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        indices: An integer, list of integers or slice(n,m) object, representing the elementary components to sum.\n",
    "        \"\"\"\n",
    "        if isinstance(indices, int): indices = [indices]\n",
    "        \n",
    "        ts_vals = self.TS_comps[:,indices].sum(axis=1)\n",
    "        return pd.Series(ts_vals, index=self.orig_TS.index)\n",
    "    \n",
    "    def calc_wcorr(self):\n",
    "        \"\"\"\n",
    "        Calculates the w-correlation matrix for the time series.\n",
    "        \"\"\"\n",
    "             \n",
    "        # Calculate the weights\n",
    "        w = np.array(list(np.arange(self.L)+1) + [self.L]*(self.K-self.L-1) + list(np.arange(self.L)+1)[::-1])\n",
    "        \n",
    "        def w_inner(F_i, F_j):\n",
    "            return w.dot(F_i*F_j)\n",
    "        \n",
    "        # Calculated weighted norms, ||F_i||_w, then invert.\n",
    "        F_wnorms = np.array([w_inner(self.TS_comps[:,i], self.TS_comps[:,i]) for i in range(self.d)])\n",
    "        F_wnorms = F_wnorms**-0.5\n",
    "        \n",
    "        # Calculate Wcorr.\n",
    "        self.Wcorr = np.identity(self.d)\n",
    "        for i in range(self.d):\n",
    "            for j in range(i+1,self.d):\n",
    "                self.Wcorr[i,j] = abs(w_inner(self.TS_comps[:,i], self.TS_comps[:,j]) * F_wnorms[i] * F_wnorms[j])\n",
    "                self.Wcorr[j,i] = self.Wcorr[i,j]\n",
    "    \n",
    "    def plot_wcorr(self, min=None, max=None):\n",
    "        \"\"\"\n",
    "        Plots the w-correlation matrix for the decomposed time series.\n",
    "        \"\"\"\n",
    "        if min is None:\n",
    "            min = 0\n",
    "        if max is None:\n",
    "            max = self.d\n",
    "        \n",
    "        if self.Wcorr is None:\n",
    "            self.calc_wcorr()\n",
    "        \n",
    "        ax = plt.imshow(self.Wcorr,interpolation = 'none')\n",
    "        plt.xlabel(r\"$\\tilde{F}_i$\")\n",
    "        plt.ylabel(r\"$\\tilde{F}_j$\")\n",
    "        plt.colorbar(ax.colorbar, fraction=0.045)\n",
    "        ax.colorbar.set_label(\"$W_{i,j}$\")\n",
    "        plt.clim(0,1)\n",
    "        \n",
    "        # For plotting purposes:\n",
    "        if max == self.d:\n",
    "            max_rnge = self.d-1\n",
    "        else:\n",
    "            max_rnge = max\n",
    "        \n",
    "        plt.xlim(min-0.5, max_rnge+0.5)\n",
    "        plt.ylim(max_rnge+0.5, min-0.5)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train \n",
      "\n",
      "Now: 99.94%\n",
      "Val \n",
      "\n",
      "Now: 99.23%\n",
      "Test \n",
      "\n",
      "Now: 99.24%"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "window = 10 # SSA window == number of components\n",
    "ssa_train_data = train_data.copy()\n",
    "ssa_val_data = val_data.copy()\n",
    "ssa_test_data = test_data.copy()\n",
    "ssa_train_0 = []\n",
    "ssa_val_0 = []\n",
    "ssa_test_0 = []\n",
    "ssa_train_1 = []\n",
    "ssa_val_1 = []\n",
    "ssa_test_1 = []\n",
    "\n",
    "print(\"\\nTrain \\n\")\n",
    "for i in range(len(ssa_train_data)):\n",
    "    ssa_train_0.append(SSA(ssa_train_data[i,0,:,0], window))\n",
    "    ssa_train_1.append(SSA(ssa_train_data[i,1,:,0], window))\n",
    "    if(i%100==1):\n",
    "        sys.stdout.write(\"\\rNow: \"+str(np.round(i*100/len(ssa_train_data), 2))+\"%\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "print(\"\\nVal \\n\")\n",
    "for i in range(len(ssa_val_data)):\n",
    "    ssa_val_0.append(SSA(ssa_val_data[i,0,:,0], window))\n",
    "    ssa_val_1.append(SSA(ssa_val_data[i,1,:,0], window))\n",
    "    if(i%100==1):\n",
    "        sys.stdout.write(\"\\rNow: \"+str(np.round(i*100/len(ssa_val_data), 2))+\"%\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "print(\"\\nTest \\n\")\n",
    "for i in range(len(ssa_test_data)):\n",
    "    ssa_test_0.append(SSA(ssa_test_data[i,0,:,0], window))\n",
    "    ssa_test_1.append(SSA(ssa_test_data[i,1,:,0], window))\n",
    "    if(i%100==1):\n",
    "        sys.stdout.write(\"\\rNow: \"+str(np.round(i*100/len(ssa_test_data), 2))+\"%\")\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 1 components:\n",
      "##################\n",
      "Iteration 0\n",
      "***[RESULT]*** ACT Accuracy: 86.16\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[94.4  2.   3.1  0.5]\n",
      " [ 6.9 86.6  6.5  0. ]\n",
      " [16.   5.2 78.5  0.3]\n",
      " [ 2.5  0.3  0.  97.2]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 86.52461172833009\n",
      "##################\n",
      "##################\n",
      "Iteration 1\n",
      "***[RESULT]*** ACT Accuracy: 87.26\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[91.4  2.   6.   0.6]\n",
      " [ 8.6 83.1  8.3  0. ]\n",
      " [10.3  2.1 87.4  0.1]\n",
      " [12.1  0.1  0.  87.8]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 86.51664767445335\n",
      "##################\n",
      "##################\n",
      "Iteration 2\n",
      "***[RESULT]*** ACT Accuracy: 86.94\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[93.   2.8  2.5  1.7]\n",
      " [ 3.9 89.   7.1  0. ]\n",
      " [14.5  6.2 79.1  0.2]\n",
      " [ 1.3  0.   0.  98.6]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 87.2818183039715\n",
      "##################\n",
      "##################\n",
      "Iteration 3\n",
      "***[RESULT]*** ACT Accuracy: 88.94\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[92.7  2.   3.1  2.3]\n",
      " [ 4.9 86.   9.   0. ]\n",
      " [12.3  2.5 84.8  0.4]\n",
      " [ 1.   0.   0.  98.9]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 88.89469625450147\n",
      "##################\n",
      "##################\n",
      "Iteration 4\n",
      "***[RESULT]*** ACT Accuracy: 87.94999999999999\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[92.3  1.2  4.6  2. ]\n",
      " [ 8.5 81.5 10.   0. ]\n",
      " [13.2  1.9 84.8  0.1]\n",
      " [ 1.   0.1  0.  98.9]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 87.84003095544323\n",
      "##################\n",
      "Raw Data, F1 Scores: [0.86524612 0.86516648 0.87281818 0.88894696 0.87840031]\n",
      "Mean: 0.8741156098333992\n",
      "STD: 0.008930463284502263\n"
     ]
    }
   ],
   "source": [
    "X = train_data.copy()\n",
    "vX = val_data.copy()\n",
    "tX = test_data.copy()\n",
    "\n",
    "num_comps = 1\n",
    "print(\"With \"+str(num_comps)+\" components:\")\n",
    "for i in range(len(X)):\n",
    "    X[i,0,:,0] = ssa_train_0[i].reconstruct(list(range(0,num_comps)))\n",
    "    X[i,1,:,0] = ssa_train_1[i].reconstruct(list(range(0,num_comps)))\n",
    "\n",
    "for i in range(len(vX)):\n",
    "    vX[i,0,:,0] = ssa_val_0[i].reconstruct(list(range(0,num_comps)))\n",
    "    vX[i,1,:,0] = ssa_val_1[i].reconstruct(list(range(0,num_comps)))    \n",
    "    \n",
    "for i in range(len(tX)):\n",
    "    tX[i,0,:,0] = ssa_test_0[i].reconstruct(list(range(0,num_comps)))\n",
    "    tX[i,1,:,0] = ssa_test_1[i].reconstruct(list(range(0,num_comps)))\n",
    "\n",
    "SSA_f1 = np.zeros(5)\n",
    "for i in range(5):\n",
    "    print(\"##################\")\n",
    "    print(\"Iteration \"+str(i))\n",
    "    SSA_f1[i] = eval_act(X, Yact, vX, vYact, tX, tYact, ep)\n",
    "    print(\"##################\")\n",
    "print(\"Raw Data, F1 Scores: \"+str(SSA_f1))\n",
    "print(\"Mean: \"+str(SSA_f1.mean()))\n",
    "print(\"STD: \"+str(SSA_f1.std()))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 2 components:\n",
      "##################\n",
      "Iteration 0\n",
      "***[RESULT]*** ACT Accuracy: 89.55\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[90.3  3.4  5.9  0.3]\n",
      " [ 3.8 83.6 12.7  0. ]\n",
      " [ 7.4  3.  89.6  0. ]\n",
      " [ 4.7  0.   0.  95.3]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 89.17389046488296\n",
      "##################\n",
      "##################\n",
      "Iteration 1\n",
      "***[RESULT]*** ACT Accuracy: 89.46\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[87.9  3.4  8.4  0.3]\n",
      " [ 2.6 87.1 10.3  0. ]\n",
      " [10.5  2.3 87.2  0.1]\n",
      " [ 0.8  0.1  0.  99.2]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 89.43304721703254\n",
      "##################\n",
      "##################\n",
      "Iteration 2\n",
      "***[RESULT]*** ACT Accuracy: 89.36\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[88.8  2.1  8.9  0.2]\n",
      " [ 1.4 89.5  9.   0. ]\n",
      " [ 9.7  3.4 86.9  0. ]\n",
      " [ 4.2  0.1  0.  95.7]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 89.31178158988754\n",
      "##################\n",
      "##################\n",
      "Iteration 3\n",
      "***[RESULT]*** ACT Accuracy: 88.44\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[90.5  4.7  4.5  0.4]\n",
      " [ 1.3 92.6  6.1  0. ]\n",
      " [14.5  3.7 81.8  0.1]\n",
      " [ 0.5  1.1  0.  98.4]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 88.67816544955396\n",
      "##################\n",
      "##################\n",
      "Iteration 4\n",
      "***[RESULT]*** ACT Accuracy: 91.43\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[93.   1.7  5.2  0.1]\n",
      " [ 1.5 92.1  6.4  0. ]\n",
      " [ 6.2  4.7 89.   0. ]\n",
      " [ 1.   3.8  0.  95.2]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 91.35557623724125\n",
      "##################\n",
      "Raw Data, F1 Scores: [0.8917389  0.89433047 0.89311782 0.88678165 0.91355576]\n",
      "Mean: 0.8959049219171966\n",
      "STD: 0.00919117760752857\n"
     ]
    }
   ],
   "source": [
    "X = train_data.copy()\n",
    "vX = val_data.copy()\n",
    "tX = test_data.copy()\n",
    "\n",
    "num_comps = 2\n",
    "print(\"With \"+str(num_comps)+\" components:\")\n",
    "for i in range(len(X)):\n",
    "    X[i,0,:,0] = ssa_train_0[i].reconstruct(list(range(0,num_comps)))\n",
    "    X[i,1,:,0] = ssa_train_1[i].reconstruct(list(range(0,num_comps)))\n",
    "\n",
    "for i in range(len(vX)):\n",
    "    vX[i,0,:,0] = ssa_val_0[i].reconstruct(list(range(0,num_comps)))\n",
    "    vX[i,1,:,0] = ssa_val_1[i].reconstruct(list(range(0,num_comps)))    \n",
    "    \n",
    "for i in range(len(tX)):\n",
    "    tX[i,0,:,0] = ssa_test_0[i].reconstruct(list(range(0,num_comps)))\n",
    "    tX[i,1,:,0] = ssa_test_1[i].reconstruct(list(range(0,num_comps)))\n",
    "\n",
    "SSA_f1 = np.zeros(5)\n",
    "for i in range(5):\n",
    "    print(\"##################\")\n",
    "    print(\"Iteration \"+str(i))\n",
    "    SSA_f1[i] = eval_act(X, Yact, vX, vYact, tX, tYact, ep)\n",
    "    print(\"##################\")\n",
    "print(\"Raw Data, F1 Scores: \"+str(SSA_f1))\n",
    "print(\"Mean: \"+str(SSA_f1.mean()))\n",
    "print(\"STD: \"+str(SSA_f1.std()))            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
