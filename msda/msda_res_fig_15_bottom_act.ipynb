{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import keras \n",
    "import keras.backend as K\n",
    "from scipy.signal import resample\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from collections import Counter\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model, load_model, model_from_json \n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, Concatenate,  Dropout \n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "def get_class_weights(y):\n",
    "    counter = Counter(y)\n",
    "    majority = max(counter.values())\n",
    "    return  {cls: float(majority/count) for cls, count in counter.items()}\n",
    "\n",
    "\n",
    "\n",
    "class Estimator:\n",
    "    l2p = 0.001\n",
    "    @staticmethod\n",
    "    def early_layers(inp, fm = (1,3), hid_act_func=\"relu\"):\n",
    "        # Start\n",
    "        x = Conv2D(32, fm, padding=\"same\", kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(inp)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(1, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        \n",
    "        # 1\n",
    "        x = Conv2D(32, fm, padding=\"same\", kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(1, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def late_layers(inp, num_classes, fm = (1,3), act_func=\"softmax\", hid_act_func=\"relu\", b_name=\"Identifier\"):\n",
    "        # 2\n",
    "        x = Conv2D(32, fm, padding=\"same\", kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(inp)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D(pool_size=(1, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        \n",
    "        \n",
    "        # End\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(128, kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(32, kernel_regularizer=regularizers.l2(Estimator.l2p), activation=hid_act_func)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(num_classes, activation=act_func, name = b_name)(x)\n",
    "\n",
    "        return x\n",
    "   \n",
    "    @staticmethod\n",
    "    def build(height, width, num_classes, name, fm = (1,3), act_func=\"softmax\",hid_act_func=\"relu\"):\n",
    "        inp = Input(shape=(height, width, 1))\n",
    "        early = Estimator.early_layers(inp, fm, hid_act_func=hid_act_func)\n",
    "        late  = Estimator.late_layers(early, num_classes, fm, act_func=act_func, hid_act_func=hid_act_func)\n",
    "        model = Model(inputs=inp, outputs=late ,name=name)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_ds_infos():\n",
    "    \"\"\"\n",
    "    Read the file includes data subject information.\n",
    "    \n",
    "    Data Columns:\n",
    "    0: code [1-24]\n",
    "    1: weight [kg]\n",
    "    2: height [cm]\n",
    "    3: age [years]\n",
    "    4: gender [0:Female, 1:Male]\n",
    "    \n",
    "    Returns:\n",
    "        A pandas DataFrame that contains inforamtion about data subjects' attributes \n",
    "    \"\"\" \n",
    "\n",
    "    dss = pd.read_csv(\"data_subjects_info.csv\")\n",
    "    print(\"[INFO] -- Data subjects' information is imported.\")\n",
    "    \n",
    "    return dss\n",
    "\n",
    "def set_data_types(data_types=[\"userAcceleration\"]):\n",
    "    \"\"\"\n",
    "    Select the sensors and the mode to shape the final dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_types: A list of sensor data type from this list: [attitude, gravity, rotationRate, userAcceleration] \n",
    "\n",
    "    Returns:\n",
    "        It returns a list of columns to use for creating time-series from files.\n",
    "    \"\"\"\n",
    "    dt_list = []\n",
    "    for t in data_types:\n",
    "        if t != \"attitude\":\n",
    "            dt_list.append([t+\".x\",t+\".y\",t+\".z\"])\n",
    "        else:\n",
    "            dt_list.append([t+\".roll\", t+\".pitch\", t+\".yaw\"])\n",
    "\n",
    "    return dt_list\n",
    "\n",
    "\n",
    "def creat_time_series(dt_list, act_labels, trial_codes, mode=\"mag\", labeled=True, combine_grav_acc=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dt_list: A list of columns that shows the type of data we want.\n",
    "        act_labels: list of activites\n",
    "        trial_codes: list of trials\n",
    "        mode: It can be \"raw\" which means you want raw data\n",
    "        for every dimention of each data type,\n",
    "        [attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)].\n",
    "        or it can be \"mag\" which means you only want the magnitude for each data type: (x^2+y^2+z^2)^(1/2)\n",
    "        labeled: True, if we want a labeld dataset. False, if we only want sensor values.\n",
    "        combine_grav_acc: True, means adding each axis of gravity to  corresponding axis of userAcceleration.\n",
    "    Returns: \n",
    "        It returns a time-series of sensor data.\n",
    "    \n",
    "    \"\"\"\n",
    "    num_data_cols = len(dt_list) if mode == \"mag\" else len(dt_list*3)\n",
    "\n",
    "    if labeled:\n",
    "        dataset = np.zeros((0,num_data_cols+7)) # \"7\" --> [act, code, weight, height, age, gender, trial] \n",
    "    else:\n",
    "        dataset = np.zeros((0,num_data_cols))\n",
    "        \n",
    "    ds_list = get_ds_infos()\n",
    "    \n",
    "    print(\"[INFO] -- Creating Time-Series\")\n",
    "    for sub_id in ds_list[\"code\"]:\n",
    "        for act_id, act in enumerate(act_labels):\n",
    "            for trial in trial_codes[act_id]:\n",
    "                fname = 'A_DeviceMotion_data/'+act+'_'+str(trial)+'/sub_'+str(int(sub_id))+'.csv'\n",
    "                raw_data = pd.read_csv(fname)\n",
    "                raw_data = raw_data.drop(['Unnamed: 0'], axis=1)\n",
    "                vals = np.zeros((len(raw_data), num_data_cols))\n",
    "                \n",
    "                if combine_grav_acc:\n",
    "                    raw_data[\"userAcceleration.x\"] = raw_data[\"userAcceleration.x\"].add(raw_data[\"gravity.x\"])\n",
    "                    raw_data[\"userAcceleration.y\"] = raw_data[\"userAcceleration.y\"].add(raw_data[\"gravity.y\"])\n",
    "                    raw_data[\"userAcceleration.z\"] = raw_data[\"userAcceleration.z\"].add(raw_data[\"gravity.z\"])\n",
    "                \n",
    "                for x_id, axes in enumerate(dt_list):\n",
    "                    if mode == \"mag\":\n",
    "                        vals[:,x_id] = (raw_data[axes]**2).sum(axis=1)**0.5        \n",
    "                    else:\n",
    "                        vals[:,x_id*3:(x_id+1)*3] = raw_data[axes].values\n",
    "                    vals = vals[:,:num_data_cols]\n",
    "                if labeled:\n",
    "                    lbls = np.array([[act_id,\n",
    "                            sub_id-1,\n",
    "                            ds_list[\"weight\"][sub_id-1],\n",
    "                            ds_list[\"height\"][sub_id-1],\n",
    "                            ds_list[\"age\"][sub_id-1],\n",
    "                            ds_list[\"gender\"][sub_id-1],\n",
    "                            trial          \n",
    "                           ]]*len(raw_data))\n",
    "                    vals = np.concatenate((vals, lbls), axis=1)\n",
    "                dataset = np.append(dataset,vals, axis=0)\n",
    "    cols = []\n",
    "    for axes in dt_list:\n",
    "        if mode == \"raw\":\n",
    "            cols += axes\n",
    "        else:\n",
    "            cols += [str(axes[0][:-2])]\n",
    "            \n",
    "    if labeled:\n",
    "        cols += [\"act\", \"id\", \"weight\", \"height\", \"age\", \"gender\", \"trial\"]\n",
    "    \n",
    "    dataset = pd.DataFrame(data=dataset, columns=cols)\n",
    "    return dataset\n",
    "#________________________________\n",
    "#________________________________\n",
    "\n",
    "def ts_to_secs(dataset, w, s, standardize = False, **options):\n",
    "    \n",
    "    data = dataset[dataset.columns[:-7]].values    \n",
    "    act_labels = dataset[\"act\"].values\n",
    "    id_labels = dataset[\"id\"].values\n",
    "    trial_labels = dataset[\"trial\"].values\n",
    "\n",
    "    mean = 0\n",
    "    std = 1\n",
    "    if standardize:\n",
    "        ## Standardize each sensorâ€™s data to have a zero mean and unity standard deviation.\n",
    "        ## As usual, we normalize test dataset by training dataset's parameters \n",
    "        if options:\n",
    "            mean = options.get(\"mean\")\n",
    "            std = options.get(\"std\")\n",
    "            print(\"[INFO] -- Test Data has been standardized\")\n",
    "        else:\n",
    "            mean = data.mean(axis=0)\n",
    "            std = data.std(axis=0)\n",
    "            print(\"[INFO] -- Training Data has been standardized: the mean is = \"+str(mean)+\" ; and the std is = \"+str(std))            \n",
    "\n",
    "        data -= mean\n",
    "        data /= std\n",
    "    else:\n",
    "        print(\"[INFO] -- Without Standardization.....\")\n",
    "\n",
    "    ## We want the Rows of matrices show each Feature and the Columns show time points.\n",
    "    data = data.T\n",
    "\n",
    "    m = data.shape[0]   # Data Dimension \n",
    "    ttp = data.shape[1] # Total Time Points\n",
    "    number_of_secs = int(round(((ttp - w)/s)))\n",
    "\n",
    "    ##  Create a 3D matrix for Storing Sections  \n",
    "    secs_data = np.zeros((number_of_secs , m , w ))\n",
    "    act_secs_labels = np.zeros(number_of_secs)\n",
    "    id_secs_labels = np.zeros(number_of_secs)\n",
    "\n",
    "    k=0\n",
    "    for i in range(0 , ttp-w, s):\n",
    "        j = i // s\n",
    "        if j >= number_of_secs:\n",
    "            break\n",
    "        if id_labels[i] != id_labels[i+w-1]: \n",
    "            continue\n",
    "        if act_labels[i] != act_labels[i+w-1]: \n",
    "            continue\n",
    "        if trial_labels[i] != trial_labels[i+w-1]:\n",
    "            continue\n",
    "            \n",
    "        secs_data[k] = data[:, i:i+w]\n",
    "        act_secs_labels[k] = act_labels[i].astype(int)\n",
    "        id_secs_labels[k] = id_labels[i].astype(int)\n",
    "        k = k+1\n",
    "        \n",
    "    secs_data = secs_data[0:k]\n",
    "    act_secs_labels = act_secs_labels[0:k]\n",
    "    id_secs_labels = id_secs_labels[0:k]\n",
    "    return secs_data, act_secs_labels, id_secs_labels, mean, std\n",
    "##________________________________________________________________\n",
    "\n",
    "\n",
    "ACT_LABELS = [\"dws\",\"ups\", \"wlk\", \"jog\", \"std\", \"sit\"]\n",
    "TRIAL_CODES = {\n",
    "    ACT_LABELS[0]:[1,2,11],\n",
    "    ACT_LABELS[1]:[3,4,12],\n",
    "    ACT_LABELS[2]:[7,8,15],\n",
    "    ACT_LABELS[3]:[9,16],\n",
    "    ACT_LABELS[4]:[6,14],\n",
    "    ACT_LABELS[5]:[5,13],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSA(object):\n",
    "    \n",
    "    __supported_types = (pd.Series, np.ndarray, list)\n",
    "    \n",
    "    def __init__(self, tseries, L, save_mem=True):\n",
    "        \"\"\"\n",
    "        Decomposes the given time series with a singular-spectrum analysis. Assumes the values of the time series are\n",
    "        recorded at equal intervals.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        tseries : The original time series, in the form of a Pandas Series, NumPy array or list. \n",
    "        L : The window length. Must be an integer 2 <= L <= N/2, where N is the length of the time series.\n",
    "        save_mem : Conserve memory by not retaining the elementary matrices. Recommended for long time series with\n",
    "            thousands of values. Defaults to True.\n",
    "        \n",
    "        Note: Even if an NumPy array or list is used for the initial time series, all time series returned will be\n",
    "        in the form of a Pandas Series or DataFrame object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Tedious type-checking for the initial time series\n",
    "        if not isinstance(tseries, self.__supported_types):\n",
    "            raise TypeError(\"Unsupported time series object. Try Pandas Series, NumPy array or list.\")\n",
    "        \n",
    "        # Checks to save us from ourselves\n",
    "        self.N = len(tseries)\n",
    "        if not 2 <= L <= self.N/2:\n",
    "            raise ValueError(\"The window length must be in the interval [2, N/2].\")\n",
    "        \n",
    "        self.L = L\n",
    "        self.orig_TS = pd.Series(tseries)\n",
    "        self.K = self.N - self.L + 1\n",
    "        \n",
    "        # Embed the time series in a trajectory matrix\n",
    "        self.X = np.array([self.orig_TS.values[i:L+i] for i in range(0, self.K)]).T\n",
    "        \n",
    "        # Decompose the trajectory matrix\n",
    "        self.U, self.Sigma, VT = np.linalg.svd(self.X)\n",
    "        self.d = np.linalg.matrix_rank(self.X)\n",
    "        \n",
    "        self.TS_comps = np.zeros((self.N, self.d))\n",
    "        \n",
    "        if not save_mem:\n",
    "            # Construct and save all the elementary matrices\n",
    "            self.X_elem = np.array([ self.Sigma[i]*np.outer(self.U[:,i], VT[i,:]) for i in range(self.d) ])\n",
    "\n",
    "            # Diagonally average the elementary matrices, store them as columns in array.           \n",
    "            for i in range(self.d):\n",
    "                X_rev = self.X_elem[i, ::-1]\n",
    "                self.TS_comps[:,i] = [X_rev.diagonal(j).mean() for j in range(-X_rev.shape[0]+1, X_rev.shape[1])]\n",
    "            \n",
    "            self.V = VT.T\n",
    "        else:\n",
    "            # Reconstruct the elementary matrices without storing them\n",
    "            for i in range(self.d):\n",
    "                X_elem = self.Sigma[i]*np.outer(self.U[:,i], VT[i,:])\n",
    "                X_rev = X_elem[::-1]\n",
    "                self.TS_comps[:,i] = [X_rev.diagonal(j).mean() for j in range(-X_rev.shape[0]+1, X_rev.shape[1])]\n",
    "            \n",
    "            self.X_elem = \"Re-run with save_mem=False to retain the elementary matrices.\"\n",
    "            \n",
    "            # The V array may also be very large under these circumstances, so we won't keep it.\n",
    "            self.V = \"Re-run with save_mem=False to retain the V matrix.\"\n",
    "        \n",
    "        # Calculate the w-correlation matrix.\n",
    "        self.calc_wcorr()\n",
    "            \n",
    "    def components_to_df(self, n=0):\n",
    "        \"\"\"\n",
    "        Returns all the time series components in a single Pandas DataFrame object.\n",
    "        \"\"\"\n",
    "        if n > 0:\n",
    "            n = min(n, self.d)\n",
    "        else:\n",
    "            n = self.d\n",
    "        \n",
    "        # Create list of columns - call them F0, F1, F2, ...\n",
    "        cols = [\"F{}\".format(i) for i in range(n)]\n",
    "        return pd.DataFrame(self.TS_comps[:, :n], columns=cols, index=self.orig_TS.index)\n",
    "            \n",
    "    \n",
    "    def reconstruct(self, indices):\n",
    "        \"\"\"\n",
    "        Reconstructs the time series from its elementary components, using the given indices. Returns a Pandas Series\n",
    "        object with the reconstructed time series.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        indices: An integer, list of integers or slice(n,m) object, representing the elementary components to sum.\n",
    "        \"\"\"\n",
    "        if isinstance(indices, int): indices = [indices]\n",
    "        \n",
    "        ts_vals = self.TS_comps[:,indices].sum(axis=1)\n",
    "        return pd.Series(ts_vals, index=self.orig_TS.index)\n",
    "    \n",
    "    def calc_wcorr(self):\n",
    "        \"\"\"\n",
    "        Calculates the w-correlation matrix for the time series.\n",
    "        \"\"\"\n",
    "             \n",
    "        # Calculate the weights\n",
    "        w = np.array(list(np.arange(self.L)+1) + [self.L]*(self.K-self.L-1) + list(np.arange(self.L)+1)[::-1])\n",
    "        \n",
    "        def w_inner(F_i, F_j):\n",
    "            return w.dot(F_i*F_j)\n",
    "        \n",
    "        # Calculated weighted norms, ||F_i||_w, then invert.\n",
    "        F_wnorms = np.array([w_inner(self.TS_comps[:,i], self.TS_comps[:,i]) for i in range(self.d)])\n",
    "        F_wnorms = F_wnorms**-0.5\n",
    "        \n",
    "        # Calculate Wcorr.\n",
    "        self.Wcorr = np.identity(self.d)\n",
    "        for i in range(self.d):\n",
    "            for j in range(i+1,self.d):\n",
    "                self.Wcorr[i,j] = abs(w_inner(self.TS_comps[:,i], self.TS_comps[:,j]) * F_wnorms[i] * F_wnorms[j])\n",
    "                self.Wcorr[j,i] = self.Wcorr[i,j]\n",
    "    \n",
    "    def plot_wcorr(self, min=None, max=None):\n",
    "        \"\"\"\n",
    "        Plots the w-correlation matrix for the decomposed time series.\n",
    "        \"\"\"\n",
    "        if min is None:\n",
    "            min = 0\n",
    "        if max is None:\n",
    "            max = self.d\n",
    "        \n",
    "        if self.Wcorr is None:\n",
    "            self.calc_wcorr()\n",
    "        \n",
    "        ax = plt.imshow(self.Wcorr,interpolation = 'none')\n",
    "        plt.xlabel(r\"$\\tilde{F}_i$\")\n",
    "        plt.ylabel(r\"$\\tilde{F}_j$\")\n",
    "        plt.colorbar(ax.colorbar, fraction=0.045)\n",
    "        ax.colorbar.set_label(\"$W_{i,j}$\")\n",
    "        plt.clim(0,1)\n",
    "        \n",
    "        # For plotting purposes:\n",
    "        if max == self.d:\n",
    "            max_rnge = self.d-1\n",
    "        else:\n",
    "            max_rnge = max\n",
    "        \n",
    "        plt.xlim(min-0.5, max_rnge+0.5)\n",
    "        plt.ylim(max_rnge+0.5, min-0.5)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/45305384/5210098\n",
    "def f1_metric(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -- Selected sensor data types: ['rotationRate', 'userAcceleration'] -- Mode: mag -- Grav+Acc: True\n",
      "[INFO] -- Selected activites: ['dws', 'ups', 'wlk', 'jog']\n",
      "[INFO] -- Data subjects' information is imported.\n",
      "[INFO] -- Creating Time-Series\n",
      "[INFO] -- Shape of time-Series dataset:(767660, 9)\n",
      "[INFO] -- Test IDs: [4, 9, 11, 21]\n",
      "[INFO] -- Shape of Train Time-Series :(646207, 9)\n",
      "[INFO] -- Shape of Test Time-Series :(121453, 9)\n",
      "___________Train_VAL____________\n",
      "[INFO] -- Training Time-Series :(523129, 9)\n",
      "[INFO] -- Validation Time-Series :(123078, 9)\n",
      "___________________________________________________\n",
      "   rotationRate  userAcceleration  act   id  weight  height   age  gender  \\\n",
      "0      1.370498          1.195847  0.0  0.0   102.0   188.0  46.0     1.0   \n",
      "1      1.141648          1.196990  0.0  0.0   102.0   188.0  46.0     1.0   \n",
      "2      0.372530          1.117437  0.0  0.0   102.0   188.0  46.0     1.0   \n",
      "3      1.049628          1.088320  0.0  0.0   102.0   188.0  46.0     1.0   \n",
      "4      0.921229          1.390551  0.0  0.0   102.0   188.0  46.0     1.0   \n",
      "\n",
      "   trial  \n",
      "0    1.0  \n",
      "1    1.0  \n",
      "2    1.0  \n",
      "3    1.0  \n",
      "4    1.0  \n",
      "[INFO] -- Training Data has been standardized: the mean is = [2.17728825 1.19431016] ; and the std is = [1.43229632 0.70168121]\n",
      "[INFO] -- Test Data has been standardized\n",
      "[INFO] -- Test Data has been standardized\n",
      "[INFO] -- Training Sections: (50532, 2, 128)\n",
      "[INFO] -- Validation Sections: (11288, 2, 128)\n",
      "[INFO] -- Test Sections:  (11589, 2, 128)\n"
     ]
    }
   ],
   "source": [
    "## Here we set parameter to build labeld time-series from dataset of \"(A)DeviceMotion_data\"\n",
    "## attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)\n",
    "results ={}\n",
    "sdt = [\"rotationRate\",\"userAcceleration\"]\n",
    "mode = \"mag\"\n",
    "cga = True # Add gravity to acceleration or not\n",
    "\n",
    "print(\"[INFO] -- Selected sensor data types: \"+str(sdt)+\" -- Mode: \"+str(mode)+\" -- Grav+Acc: \"+str(cga))    \n",
    "act_labels = ACT_LABELS [0:4]\n",
    "\n",
    "print(\"[INFO] -- Selected activites: \"+str(act_labels))    \n",
    "trial_codes = [TRIAL_CODES[act] for act in act_labels]\n",
    "dt_list = set_data_types(sdt)\n",
    "dataset = creat_time_series(dt_list, act_labels, trial_codes, mode=mode, labeled=True, combine_grav_acc = cga)\n",
    "print(\"[INFO] -- Shape of time-Series dataset:\"+str(dataset.shape))    \n",
    "\n",
    "\n",
    "#*****************\n",
    "TRAIN_TEST_TYPE = \"subject\" # \"subject\" or \"trial\"\n",
    "#*****************\n",
    "\n",
    "if TRAIN_TEST_TYPE == \"subject\":\n",
    "    test_ids = [4,9,11,21]\n",
    "    print(\"[INFO] -- Test IDs: \"+str(test_ids))\n",
    "    test_ts = dataset.loc[(dataset['id'].isin(test_ids))]\n",
    "    train_ts = dataset.loc[~(dataset['id'].isin(test_ids))]\n",
    "else:\n",
    "    test_trail = [11,12,13,14,15,16]  \n",
    "    print(\"[INFO] -- Test Trials: \"+str(test_trail))\n",
    "    test_ts = dataset.loc[(dataset['trial'].isin(test_trail))]\n",
    "    train_ts = dataset.loc[~(dataset['trial'].isin(test_trail))]\n",
    "\n",
    "print(\"[INFO] -- Shape of Train Time-Series :\"+str(train_ts.shape))\n",
    "print(\"[INFO] -- Shape of Test Time-Series :\"+str(test_ts.shape))\n",
    "\n",
    "print(\"___________Train_VAL____________\")\n",
    "val_trail = [11,12,13,14,15,16]\n",
    "val_ts = train_ts.loc[(train_ts['trial'].isin(val_trail))]\n",
    "train_ts = train_ts.loc[~(train_ts['trial'].isin(val_trail))]\n",
    "print(\"[INFO] -- Training Time-Series :\"+str(train_ts.shape))\n",
    "print(\"[INFO] -- Validation Time-Series :\"+str(val_ts.shape))\n",
    "\n",
    "print(\"___________________________________________________\")\n",
    "print(train_ts.head())\n",
    "\n",
    "## This Variable Defines the Size of Sliding Window\n",
    "## ( e.g. 100 means in each snapshot we just consider 100 consecutive observations of each sensor) \n",
    "w = 128 # 50 Equals to 1 second for MotionSense Dataset (it is on 50Hz samplig rate)\n",
    "## Here We Choose Step Size for Building Diffrent Snapshots from Time-Series Data\n",
    "## ( smaller step size will increase the amount of the instances and higher computational cost may be incurred )\n",
    "s = 10\n",
    "train_data, act_train, id_train, train_mean, train_std = ts_to_secs(train_ts.copy(),\n",
    "                                                                   w,\n",
    "                                                                   s,\n",
    "                                                                   standardize = True)\n",
    "\n",
    "s = 10\n",
    "val_data, act_val, id_val, val_mean, val_std = ts_to_secs(val_ts.copy(),\n",
    "                                                          w,\n",
    "                                                          s,\n",
    "                                                          standardize = True,\n",
    "                                                          mean = train_mean, \n",
    "                                                          std = train_std)\n",
    "\n",
    "s = 10\n",
    "test_data, act_test, id_test, test_mean, test_std = ts_to_secs(test_ts.copy(),\n",
    "                                                              w,\n",
    "                                                              s,\n",
    "                                                              standardize = True,\n",
    "                                                              mean = train_mean, \n",
    "                                                              std = train_std)\n",
    "\n",
    "print(\"[INFO] -- Training Sections: \"+str(train_data.shape))\n",
    "print(\"[INFO] -- Validation Sections: \"+str(val_data.shape))\n",
    "print(\"[INFO] -- Test Sections:  \"+str(test_data.shape))\n",
    "\n",
    "\n",
    "id_train_labels = to_categorical(id_train)\n",
    "id_val_labels = to_categorical(id_val)\n",
    "id_test_labels = to_categorical(id_test)\n",
    "\n",
    "act_train_labels = to_categorical(act_train)\n",
    "act_val_labels = to_categorical(act_val)\n",
    "act_test_labels = to_categorical(act_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[INFO] -- Shape of Training Sections:', (50532, 2, 128, 1))\n",
      "[INFO] -- Validation Sections:(11288, 2, 128, 1)\n",
      "('[INFO] -- Shape of Training Sections:', (11589, 2, 128, 1))\n"
     ]
    }
   ],
   "source": [
    "## Here we add an extra dimension to the datasets just to be ready for using with Convolution2D\n",
    "train_data = np.expand_dims(train_data,axis=3)\n",
    "print(\"[INFO] -- Shape of Training Sections:\", train_data.shape)\n",
    "val_data = np.expand_dims(val_data,axis=3)\n",
    "print(\"[INFO] -- Validation Sections:\"+str(val_data.shape))\n",
    "test_data = np.expand_dims(test_data,axis=3)\n",
    "print(\"[INFO] -- Shape of Training Sections:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train \n",
      "\n",
      "Now: 99%\n",
      " Val \n",
      "\n",
      "Now: 99%\n",
      " Test \n",
      "\n",
      "Now: 99%"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "window = 10 # SSA window == number of components\n",
    "ssa_train_data = train_data.copy()\n",
    "ssa_val_data = val_data.copy()\n",
    "ssa_test_data = test_data.copy()\n",
    "ssa_train_0 = []\n",
    "ssa_train_1 = []\n",
    "ssa_val_0 = []\n",
    "ssa_val_1 = []\n",
    "ssa_test_0 = []\n",
    "ssa_test_1 = []\n",
    "\n",
    "print(\"\\n Train \\n\")\n",
    "for i in range(len(ssa_train_data)):\n",
    "    ssa_train_0.append(SSA(ssa_train_data[i,0,:,0], window))\n",
    "    ssa_train_1.append(SSA(ssa_train_data[i,1,:,0], window))\n",
    "    if(i%100==1):\n",
    "        sys.stdout.write(\"\\rNow: \"+str(np.round(i*100/len(ssa_train_data), 2))+\"%\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "print(\"\\n Val \\n\")\n",
    "for i in range(len(ssa_val_data)):\n",
    "    ssa_val_0.append(SSA(ssa_val_data[i,0,:,0], window))\n",
    "    ssa_val_1.append(SSA(ssa_val_data[i,1,:,0], window))\n",
    "    if(i%100==1):\n",
    "        sys.stdout.write(\"\\rNow: \"+str(np.round(i*100/len(ssa_val_data), 2))+\"%\")\n",
    "        sys.stdout.flush()        \n",
    "\n",
    "print(\"\\n Test \\n\")\n",
    "for i in range(len(ssa_test_data)):\n",
    "    ssa_test_0.append(SSA(ssa_test_data[i,0,:,0], window))\n",
    "    ssa_test_1.append(SSA(ssa_test_data[i,1,:,0], window))\n",
    "    if(i%100==1):\n",
    "        sys.stdout.write(\"\\rNow: \"+str(np.round(i*100/len(ssa_test_data), 2))+\"%\")\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 1 components:\n",
      "___________________________________________________\n",
      "Model Size = 157380\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.89476, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.89476 to 0.94622, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00003: val_f1_metric improved from 0.94622 to 0.96358, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00004: val_f1_metric did not improve from 0.96358\n",
      "\n",
      "Epoch 00005: val_f1_metric improved from 0.96358 to 0.97109, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00006: val_f1_metric improved from 0.97109 to 0.97146, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00007: val_f1_metric improved from 0.97146 to 0.97155, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00008: val_f1_metric improved from 0.97155 to 0.97838, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00009: val_f1_metric improved from 0.97838 to 0.98211, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.98211\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.98211\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.98211\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.98211\n",
      "\n",
      "Epoch 00014: val_f1_metric did not improve from 0.98211\n",
      "\n",
      "Epoch 00015: val_f1_metric did not improve from 0.98211\n",
      "\n",
      "Epoch 00016: val_f1_metric did not improve from 0.98211\n",
      "***[RESULT]*** ACT Accuracy: 84.08\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[89.   3.1  6.9  1.1]\n",
      " [ 5.2 87.7  7.1  0. ]\n",
      " [11.7  8.  80.1  0.2]\n",
      " [12.3  2.3  0.  85.4]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 0.8373007840593577\n",
      "With 2 components:\n",
      "___________________________________________________\n",
      "Model Size = 157380\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.95945, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.95945 to 0.97691, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00003: val_f1_metric improved from 0.97691 to 0.98140, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00004: val_f1_metric did not improve from 0.98140\n",
      "\n",
      "Epoch 00005: val_f1_metric improved from 0.98140 to 0.98936, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00006: val_f1_metric did not improve from 0.98936\n",
      "\n",
      "Epoch 00007: val_f1_metric did not improve from 0.98936\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.98936\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.98936\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.98936\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.98936\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.98936\n",
      "***[RESULT]*** ACT Accuracy: 89.75\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[89.4  2.   8.4  0.2]\n",
      " [ 2.  89.7  8.3  0. ]\n",
      " [10.6  2.7 86.7  0. ]\n",
      " [ 2.4  0.   0.  97.6]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 0.8978451800692291\n",
      "With 3 components:\n",
      "___________________________________________________\n",
      "Model Size = 157380\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.95898, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00002: val_f1_metric did not improve from 0.95898\n",
      "\n",
      "Epoch 00003: val_f1_metric improved from 0.95898 to 0.97656, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00004: val_f1_metric improved from 0.97656 to 0.98029, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00005: val_f1_metric did not improve from 0.98029\n",
      "\n",
      "Epoch 00006: val_f1_metric improved from 0.98029 to 0.98487, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00007: val_f1_metric improved from 0.98487 to 0.99021, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.99021\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.99021\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.99021\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.99021\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.99021\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.99021\n",
      "\n",
      "Epoch 00014: val_f1_metric did not improve from 0.99021\n",
      "***[RESULT]*** ACT Accuracy: 92.47999999999999\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[93.8  1.   4.6  0.6]\n",
      " [ 1.1 90.3  8.6  0. ]\n",
      " [ 7.7  2.  90.3  0. ]\n",
      " [ 0.7  0.2  0.  99.1]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 0.9252915369606807\n",
      "With 4 components:\n",
      "___________________________________________________\n",
      "Model Size = 157380\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.97404, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00002: val_f1_metric did not improve from 0.97404\n",
      "\n",
      "Epoch 00003: val_f1_metric improved from 0.97404 to 0.98551, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00004: val_f1_metric did not improve from 0.98551\n",
      "\n",
      "Epoch 00005: val_f1_metric did not improve from 0.98551\n",
      "\n",
      "Epoch 00006: val_f1_metric did not improve from 0.98551\n",
      "\n",
      "Epoch 00007: val_f1_metric improved from 0.98551 to 0.98565, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.98565\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.98565\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.98565\n",
      "\n",
      "Epoch 00011: val_f1_metric improved from 0.98565 to 0.98647, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.98647\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.98647\n",
      "\n",
      "Epoch 00014: val_f1_metric improved from 0.98647 to 0.98855, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00015: val_f1_metric did not improve from 0.98855\n",
      "\n",
      "Epoch 00016: val_f1_metric did not improve from 0.98855\n",
      "\n",
      "Epoch 00017: val_f1_metric improved from 0.98855 to 0.98927, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00018: val_f1_metric did not improve from 0.98927\n",
      "\n",
      "Epoch 00019: val_f1_metric did not improve from 0.98927\n",
      "\n",
      "Epoch 00020: val_f1_metric did not improve from 0.98927\n",
      "\n",
      "Epoch 00021: val_f1_metric did not improve from 0.98927\n",
      "\n",
      "Epoch 00022: val_f1_metric did not improve from 0.98927\n",
      "\n",
      "Epoch 00023: val_f1_metric did not improve from 0.98927\n",
      "\n",
      "Epoch 00024: val_f1_metric did not improve from 0.98927\n",
      "***[RESULT]*** ACT Accuracy: 91.79\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[96.9  0.6  1.4  1.1]\n",
      " [ 1.3 93.5  5.2  0. ]\n",
      " [12.9  1.  86.   0. ]\n",
      " [ 0.2  0.2  0.  99.6]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 0.9204775768242357\n",
      "With 5 components:\n",
      "___________________________________________________\n",
      "Model Size = 157380\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.96681, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.96681 to 0.97663, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00003: val_f1_metric improved from 0.97663 to 0.98714, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00004: val_f1_metric did not improve from 0.98714\n",
      "\n",
      "Epoch 00005: val_f1_metric did not improve from 0.98714\n",
      "\n",
      "Epoch 00006: val_f1_metric did not improve from 0.98714\n",
      "\n",
      "Epoch 00007: val_f1_metric did not improve from 0.98714\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.98714\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.98714\n",
      "\n",
      "Epoch 00010: val_f1_metric improved from 0.98714 to 0.98836, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.98836\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.98836\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.98836\n",
      "\n",
      "Epoch 00014: val_f1_metric did not improve from 0.98836\n",
      "\n",
      "Epoch 00015: val_f1_metric did not improve from 0.98836\n",
      "\n",
      "Epoch 00016: val_f1_metric did not improve from 0.98836\n",
      "\n",
      "Epoch 00017: val_f1_metric did not improve from 0.98836\n",
      "***[RESULT]*** ACT Accuracy: 87.94\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[93.3  1.9  3.4  1.4]\n",
      " [ 4.  94.2  1.7  0. ]\n",
      " [20.1  1.2 78.7  0. ]\n",
      " [ 1.   0.   0.  99. ]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 0.8852093691416925\n",
      "With 6 components:\n",
      "___________________________________________________\n",
      "Model Size = 157380\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.96956, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.96956 to 0.97979, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00003: val_f1_metric improved from 0.97979 to 0.98026, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00004: val_f1_metric improved from 0.98026 to 0.98103, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00005: val_f1_metric improved from 0.98103 to 0.98355, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00006: val_f1_metric improved from 0.98355 to 0.98487, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00007: val_f1_metric improved from 0.98487 to 0.98977, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.98977\n",
      "\n",
      "Epoch 00009: val_f1_metric improved from 0.98977 to 0.99093, saving model to XXACT.best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.99093\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.99093\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.99093\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.99093\n",
      "\n",
      "Epoch 00014: val_f1_metric did not improve from 0.99093\n",
      "\n",
      "Epoch 00015: val_f1_metric did not improve from 0.99093\n",
      "\n",
      "Epoch 00016: val_f1_metric did not improve from 0.99093\n",
      "***[RESULT]*** ACT Accuracy: 93.55\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[89.8  2.   7.5  0.7]\n",
      " [ 1.4 90.7  7.9  0. ]\n",
      " [ 5.7  0.4 93.9  0. ]\n",
      " [ 0.6  0.3  0.  99.1]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 0.9331388109011232\n",
      "With 7 components:\n",
      "___________________________________________________\n",
      "Model Size = 157380\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.96680, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.96680 to 0.97960, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00003: val_f1_metric did not improve from 0.97960\n",
      "\n",
      "Epoch 00004: val_f1_metric improved from 0.97960 to 0.98499, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00005: val_f1_metric did not improve from 0.98499\n",
      "\n",
      "Epoch 00006: val_f1_metric did not improve from 0.98499\n",
      "\n",
      "Epoch 00007: val_f1_metric did not improve from 0.98499\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.98499\n",
      "\n",
      "Epoch 00009: val_f1_metric improved from 0.98499 to 0.98585, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.98585\n",
      "\n",
      "Epoch 00011: val_f1_metric improved from 0.98585 to 0.98900, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.98900\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.98900\n",
      "\n",
      "Epoch 00014: val_f1_metric did not improve from 0.98900\n",
      "\n",
      "Epoch 00015: val_f1_metric did not improve from 0.98900\n",
      "\n",
      "Epoch 00016: val_f1_metric did not improve from 0.98900\n",
      "\n",
      "Epoch 00017: val_f1_metric did not improve from 0.98900\n",
      "\n",
      "Epoch 00018: val_f1_metric did not improve from 0.98900\n",
      "***[RESULT]*** ACT Accuracy: 93.73\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[88.8  0.5 10.6  0.2]\n",
      " [ 1.5 91.5  7.   0. ]\n",
      " [ 5.3  0.4 94.3  0. ]\n",
      " [ 0.4  0.4  0.  99.2]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 0.9356751465372335\n",
      "With 8 components:\n",
      "___________________________________________________\n",
      "Model Size = 157380\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.96612, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.96612 to 0.98462, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00003: val_f1_metric did not improve from 0.98462\n",
      "\n",
      "Epoch 00004: val_f1_metric did not improve from 0.98462\n",
      "\n",
      "Epoch 00005: val_f1_metric improved from 0.98462 to 0.98474, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00006: val_f1_metric did not improve from 0.98474\n",
      "\n",
      "Epoch 00007: val_f1_metric did not improve from 0.98474\n",
      "\n",
      "Epoch 00008: val_f1_metric improved from 0.98474 to 0.98652, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.98652\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.98652\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.98652\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.98652\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.98652\n",
      "\n",
      "Epoch 00014: val_f1_metric improved from 0.98652 to 0.98690, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00015: val_f1_metric did not improve from 0.98690\n",
      "\n",
      "Epoch 00016: val_f1_metric did not improve from 0.98690\n",
      "\n",
      "Epoch 00017: val_f1_metric did not improve from 0.98690\n",
      "\n",
      "Epoch 00018: val_f1_metric did not improve from 0.98690\n",
      "\n",
      "Epoch 00019: val_f1_metric did not improve from 0.98690\n",
      "\n",
      "Epoch 00020: val_f1_metric did not improve from 0.98690\n",
      "\n",
      "Epoch 00021: val_f1_metric improved from 0.98690 to 0.98702, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00022: val_f1_metric did not improve from 0.98702\n",
      "\n",
      "Epoch 00023: val_f1_metric did not improve from 0.98702\n",
      "\n",
      "Epoch 00024: val_f1_metric did not improve from 0.98702\n",
      "\n",
      "Epoch 00025: val_f1_metric did not improve from 0.98702\n",
      "\n",
      "Epoch 00026: val_f1_metric did not improve from 0.98702\n",
      "\n",
      "Epoch 00027: val_f1_metric did not improve from 0.98702\n",
      "\n",
      "Epoch 00028: val_f1_metric did not improve from 0.98702\n",
      "***[RESULT]*** ACT Accuracy: 90.74\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[94.4  0.4  3.7  1.5]\n",
      " [ 2.9 90.4  6.7  0. ]\n",
      " [13.   0.7 86.3  0. ]\n",
      " [ 1.   0.1  0.  98.9]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 0.9087758205570757\n",
      "With 9 components:\n",
      "___________________________________________________\n",
      "Model Size = 157380\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.94212, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.94212 to 0.97147, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00003: val_f1_metric improved from 0.97147 to 0.98070, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00004: val_f1_metric improved from 0.98070 to 0.98438, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00005: val_f1_metric improved from 0.98438 to 0.98780, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00006: val_f1_metric did not improve from 0.98780\n",
      "\n",
      "Epoch 00007: val_f1_metric improved from 0.98780 to 0.98853, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.98853\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.98853\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.98853\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.98853\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.98853\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.98853\n",
      "\n",
      "Epoch 00014: val_f1_metric did not improve from 0.98853\n",
      "***[RESULT]*** ACT Accuracy: 91.52\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[90.6  1.6  6.3  1.4]\n",
      " [ 5.5 88.2  6.2  0. ]\n",
      " [ 5.7  0.9 93.4  0. ]\n",
      " [ 8.2  0.5  0.  91.4]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 0.9059269484286512\n",
      "With 10 components:\n",
      "___________________________________________________\n",
      "Model Size = 157380\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.95472, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.95472 to 0.96041, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00003: val_f1_metric improved from 0.96041 to 0.96807, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00004: val_f1_metric improved from 0.96807 to 0.98017, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00005: val_f1_metric improved from 0.98017 to 0.98373, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00006: val_f1_metric did not improve from 0.98373\n",
      "\n",
      "Epoch 00007: val_f1_metric did not improve from 0.98373\n",
      "\n",
      "Epoch 00008: val_f1_metric improved from 0.98373 to 0.98420, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00009: val_f1_metric improved from 0.98420 to 0.98704, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00010: val_f1_metric improved from 0.98704 to 0.98873, saving model to XXACT.best.hdf5\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.98873\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.98873\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.98873\n",
      "\n",
      "Epoch 00014: val_f1_metric did not improve from 0.98873\n",
      "\n",
      "Epoch 00015: val_f1_metric did not improve from 0.98873\n",
      "\n",
      "Epoch 00016: val_f1_metric did not improve from 0.98873\n",
      "\n",
      "Epoch 00017: val_f1_metric did not improve from 0.98873\n",
      "***[RESULT]*** ACT Accuracy: 94.85\n",
      "***[RESULT]*** ACT  Confusion Matrix\n",
      "[[91.7  0.9  7.1  0.3]\n",
      " [ 4.1 91.7  4.2  0. ]\n",
      " [ 4.   0.5 95.5  0. ]\n",
      " [ 0.4  0.1  0.  99.5]]\n",
      "***[RESULT]*** ACT Averaged F-1 Score : 0.9448430171859529\n"
     ]
    }
   ],
   "source": [
    "act_history = {}\n",
    "ep = 32\n",
    "for num_comps in range(1,11):\n",
    "    ssa_train_data = train_data.copy()\n",
    "    ssa_val_data = val_data.copy()\n",
    "    ssa_test_data = test_data.copy()\n",
    "    \n",
    "    print(\"With \"+str(num_comps)+\" components:\")\n",
    "    for i in range(len(ssa_train_data)):\n",
    "        ssa_train_data[i,0,:,0] = ssa_train_0[i].reconstruct(list(range(0,num_comps)))\n",
    "        ssa_train_data[i,1,:,0] = ssa_train_1[i].reconstruct(list(range(0,num_comps)))\n",
    "    for i in range(len(ssa_val_data)):\n",
    "        ssa_val_data[i,0,:,0] = ssa_val_0[i].reconstruct(list(range(0,num_comps)))\n",
    "        ssa_val_data[i,1,:,0] = ssa_val_1[i].reconstruct(list(range(0,num_comps)))    \n",
    "    for i in range(len(ssa_test_data)):\n",
    "        ssa_test_data[i,0,:,0] = ssa_test_0[i].reconstruct(list(range(0,num_comps)))\n",
    "        ssa_test_data[i,1,:,0] = ssa_test_1[i].reconstruct(list(range(0,num_comps)))\n",
    "    \n",
    "    height = train_data.shape[1]\n",
    "    width = train_data.shape[2]\n",
    "\n",
    "    id_class_numbers = 24\n",
    "    act_class_numbers = 4\n",
    "    fm = (2,5)\n",
    "\n",
    "    print(\"___________________________________________________\")\n",
    "    ## Callbacks\n",
    "    eval_metric= \"val_f1_metric\"    \n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor=eval_metric, mode='max', patience = 7)\n",
    "    filepath=\"XXACT.best.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor=eval_metric, verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint, early_stop]\n",
    "    ## Callbacks\n",
    "\n",
    "    eval_act = Estimator.build(height, width, act_class_numbers, name =\"EVAL_ACT\", fm=fm, act_func=\"softmax\",hid_act_func=\"relu\")\n",
    "    eval_act.compile( loss=\"categorical_crossentropy\", optimizer='adam', metrics=['acc',f1_metric])\n",
    "    print(\"Model Size = \"+str(eval_act.count_params()))\n",
    "    \n",
    "    eval_act.fit(ssa_train_data, act_train_labels,\n",
    "                validation_data = (ssa_val_data, act_val_labels),\n",
    "                epochs = ep,\n",
    "                batch_size = 128,\n",
    "                verbose = 0,\n",
    "                class_weight = get_class_weights(np.argmax(act_train_labels,axis=1)),\n",
    "                callbacks = callbacks_list\n",
    "               )\n",
    "\n",
    "    eval_act.load_weights(\"XXACT.best.hdf5\")\n",
    "    eval_act.compile( loss=\"categorical_crossentropy\", optimizer='adam', metrics=['acc',f1_metric])\n",
    "\n",
    "    result1 = eval_act.evaluate(ssa_test_data, act_test_labels, verbose = 2)\n",
    "    act_acc = result1[1].round(4)*100\n",
    "    print(\"***[RESULT]*** ACT Accuracy: \"+str(act_acc))\n",
    "\n",
    "    preds = eval_act.predict(ssa_test_data)\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    conf_mat = confusion_matrix(np.argmax(act_test_labels, axis=1), preds)\n",
    "    conf_mat = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "    print(\"***[RESULT]*** ACT  Confusion Matrix\")\n",
    "    print(np.array(conf_mat).round(3)*100)  \n",
    "\n",
    "    f1act = f1_score(np.argmax(act_test_labels, axis=1), preds, average=None).mean()\n",
    "    print(\"***[RESULT]*** ACT Averaged F-1 Score : \"+str(f1act))\n",
    "    act_history[num_comps] = f1act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.8373007840593577,\n",
       " 2: 0.8978451800692291,\n",
       " 3: 0.9252915369606807,\n",
       " 4: 0.9204775768242357,\n",
       " 5: 0.8852093691416925,\n",
       " 6: 0.9331388109011232,\n",
       " 7: 0.9356751465372335,\n",
       " 8: 0.9087758205570757,\n",
       " 9: 0.9059269484286512,\n",
       " 10: 0.9448430171859529}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
